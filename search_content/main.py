from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict
from openai import OpenAI
import requests
import os
from dotenv import load_dotenv
from datetime import datetime

# Cargar variables de entorno
load_dotenv()

app = FastAPI()

# Configurar CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Configurar clientes API
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
SONAR_API_KEY = os.getenv("SONAR_API_KEY")

# Modelos de datos
class PropuestaLaboralTexto(BaseModel):
    texto: str

class PropuestaLaboral(BaseModel):
    empresa: str
    puesto: str
    descripcion: str
    requisitos: str

class SonarResponse(BaseModel):
    contenido: str
    fuentes: List[Dict]
    busquedas_realizadas: bool
    tiempo_respuesta: float
    modelo_usado: str

class RespuestaEntrevista(BaseModel):
    preguntas: List[str]
    informacion_empresa: Dict
    propuesta_extraida: Dict
    investigacion_detallada: Dict

# Funci√≥n para extraer informaci√≥n de la propuesta
def extraer_informacion_propuesta(texto: str) -> PropuestaLaboral:
    """Extrae informaci√≥n estructurada de un texto de propuesta laboral usando OpenAI"""
    
    prompt = f"""
    Analiza el siguiente texto de una propuesta laboral y extrae la informaci√≥n estructurada.
    
    Texto de la propuesta:
    {texto}
    
    Extrae y responde en formato JSON con las siguientes claves:
    - "empresa": nombre de la empresa
    - "puesto": t√≠tulo del puesto
    - "descripcion": descripci√≥n del trabajo y responsabilidades
    - "requisitos": concatena toda la informaci√≥n de requisitos, formaci√≥n, experiencia y competencias en un solo texto
    
    Para el campo "requisitos", incluye toda la informaci√≥n relevante en un p√°rrafo continuo.
    """
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Eres un experto en an√°lisis de propuestas laborales. Siempre responde en formato JSON v√°lido con los campos exactos solicitados."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            response_format={"type": "json_object"}
        )
        
        import json
        datos = json.loads(response.choices[0].message.content)
        
        print(f"‚úÖ Datos extra√≠dos: {datos}")
        
        return PropuestaLaboral(**datos)
    except Exception as e:
        print(f"‚ùå Error extrayendo informaci√≥n: {e}")
        return PropuestaLaboral(
            empresa="Empresa no identificada",
            puesto="Puesto no identificado", 
            descripcion=texto[:200],
            requisitos="No especificados"
        )

# Funci√≥n optimizada para b√∫squedas con Sonar - SIEMPRE M√ÅXIMA CALIDAD
def buscar_con_sonar(query: str, search_type: str = "pro") -> SonarResponse:
    """Busca informaci√≥n usando la API de Sonar de Perplexity con M√ÅXIMA CALIDAD siempre"""
    
    start_time = datetime.now()
    url = "https://api.perplexity.ai/chat/completions"
    
    headers = {
        "Authorization": f"Bearer {SONAR_API_KEY}",
        "Content-Type": "application/json"
    }
    
    # Configuraci√≥n SIEMPRE de m√°xima calidad
    modelo = "sonar-pro"
    max_tokens = 2500  # Aumentado para m√°xima calidad
    temperature = 0.1  # M√°s preciso para b√∫squedas espec√≠ficas
    
    payload = {
        "model": modelo,
        "messages": [
            {
                "role": "system",
                "content": "Eres un investigador experto de √©lite que proporciona informaci√≥n actualizada, precisa y extremadamente detallada basada en fuentes web confiables de alta calidad. Prioriza fuentes oficiales, verificables y recientes. Incluye detalles espec√≠ficos, datos concretos y contexto relevante en cada respuesta."
            },
            {
                "role": "user", 
                "content": query
            }
        ],
        "max_tokens": max_tokens,
        "temperature": temperature,
        # "search_domain_filter": []  # Permitir b√∫squedas en toda la web
        "search_recency_filter": "month",  # √öltimas b√∫squedas del mes
        "return_related_questions": True,
        "search_depth_filter": "advanced",  # B√∫squeda m√°s profunda
        "enable_clarification_questions": True  # Preguntas de clarificaci√≥n para mejor contexto
    }
    
    try:
        if not SONAR_API_KEY:
            print("‚ùå SONAR_API_KEY no configurada")
            return SonarResponse(
                contenido="Informaci√≥n no disponible - API key no configurada",
                fuentes=[],
                busquedas_realizadas=False,
                tiempo_respuesta=0.0,
                modelo_usado="none"
            )
            
        print(f"üîç Iniciando b√∫squeda con {modelo} (M√ÅXIMA CALIDAD)")
        print(f"üìù Prompt enviado: {query[:150]}...")
        print(f"‚ö° Configuraci√≥n: {max_tokens} tokens, temp={temperature}")
        print("‚è±Ô∏è  Esperando respuesta...")
        
        response = requests.post(url, json=payload, headers=headers, timeout=45)
        tiempo_respuesta = (datetime.now() - start_time).total_seconds()
        
        if response.status_code == 200:
            data = response.json()
            print(f"üîç Debug - Claves en respuesta: {list(data.keys())}")
            
            # Validar estructura de respuesta
            if "choices" not in data or not data["choices"]:
                print("‚ùå Error: Respuesta sin campo 'choices'")
                return SonarResponse(
                    contenido="Error en estructura de respuesta",
                    fuentes=[],
                    busquedas_realizadas=False,
                    tiempo_respuesta=tiempo_respuesta,
                    modelo_usado=modelo
                )
            
            content = data["choices"][0]["message"]["content"]
            
            # Mostrar el contenido de la respuesta en consola
            print("üìÑ CONTENIDO DE LA B√öSQUEDA:")
            print(f"{'='*80}")
            print(content[:500] + "..." if len(content) > 500 else content)
            print(f"{'='*80}")
            
            # Extraer fuentes/citas de la respuesta
            fuentes = []
            
            # Debug: mostrar estructura de datos adicionales (limitado a 3 fuentes)
            if "search_results" in data:
                print(f"üîç Debug - Encontrados {len(data['search_results'])} resultados (usando m√°ximo 3)")
                if data["search_results"]:
                    print(f"üîç Debug - Primer tipo de resultado: {type(data['search_results'][0])}")
            
            if "citations" in data:
                print(f"üîç Debug - Encontradas {len(data['citations'])} citas (usando m√°ximo 3)")
                if data["citations"]:
                    print(f"üîç Debug - Primer tipo de cita: {type(data['citations'][0])}")
            
            # Extraer fuentes de search_results (limitado a 3 fuentes)
            if "search_results" in data and data["search_results"]:
                for i, result in enumerate(data["search_results"][:3]):  # M√°ximo 3 fuentes
                    if isinstance(result, dict):
                        fuentes.append({
                            "numero": i + 1,
                            "titulo": result.get("title", result.get("name", "T√≠tulo no disponible"))[:80] + "..." if len(result.get("title", result.get("name", ""))) > 80 else result.get("title", result.get("name", "T√≠tulo no disponible")),
                            "url": result.get("url", "URL no disponible"),
                            "fecha": result.get("date", result.get("published_date", "Fecha no disponible"))
                        })
                    else:
                        # Si el resultado es un string u otro tipo
                        fuentes.append({
                            "numero": i + 1,
                            "titulo": str(result)[:80] + "..." if len(str(result)) > 80 else str(result),
                            "url": "URL no disponible",
                            "fecha": "Fecha no disponible"
                        })
            
            # Si no hay search_results, intentar con citations (limitado a 3)
            elif "citations" in data and data["citations"]:
                for i, citation in enumerate(data["citations"][:3]):  # M√°ximo 3 citas
                    if isinstance(citation, dict):
                        fuentes.append({
                            "numero": i + 1,
                            "titulo": citation.get("title", "T√≠tulo no disponible")[:80] + "..." if len(citation.get("title", "")) > 80 else citation.get("title", "T√≠tulo no disponible"),
                            "url": citation.get("url", "URL no disponible"),
                            "fecha": citation.get("date", "Fecha no disponible")
                        })
                    else:
                        # Las citas como strings suelen ser URLs
                        url_citation = str(citation)
                        fuentes.append({
                            "numero": i + 1,
                            "titulo": f"Fuente {i + 1}",
                            "url": url_citation,
                            "fecha": "Fecha no disponible"
                        })
            
            # Mostrar fuentes encontradas en detalle (m√°ximo 3)
            if fuentes:
                print(f"üìö FUENTES ENCONTRADAS ({len(fuentes)}/3):")
                print(f"{'-'*80}")
                for fuente in fuentes:
                    print(f"  {fuente['numero']}. {fuente['titulo']}")
                    if fuente['url'] != "URL no disponible":
                        print(f"     üîó {fuente['url']}")
                    if fuente['fecha'] != "Fecha no disponible":
                        print(f"     üìÖ {fuente['fecha']}")
                    print()
                print(f"{'-'*80}")
            else:
                print("‚ö†Ô∏è  No se encontraron fuentes espec√≠ficas")
            
            # Verificar si se realizaron b√∫squedas web
            busquedas_realizadas = len(fuentes) > 0 or "based on" in content.lower() or "according to" in content.lower()
            
            print(f"‚úÖ B√∫squeda completada en {tiempo_respuesta:.2f}s con {len(fuentes)} fuentes")
            
            return SonarResponse(
                contenido=content,
                fuentes=fuentes,
                busquedas_realizadas=busquedas_realizadas,
                tiempo_respuesta=tiempo_respuesta,
                modelo_usado=modelo
            )
        else:
            print(f"‚ùå Error en Sonar - Status: {response.status_code}, Response: {response.text}")
            return SonarResponse(
                contenido="Informaci√≥n no disponible en este momento",
                fuentes=[],
                busquedas_realizadas=False,
                tiempo_respuesta=tiempo_respuesta,
                modelo_usado=modelo
            )
            
    except requests.exceptions.Timeout:
        print("‚è∞ Timeout en Sonar API")
        return SonarResponse(
            contenido="Informaci√≥n no disponible - timeout",
            fuentes=[],
            busquedas_realizadas=False,
            tiempo_respuesta=45.0,
            modelo_usado=modelo
        )
    except Exception as e:
        print(f"‚ùå Error en Sonar: {e}")
        return SonarResponse(
            contenido="Informaci√≥n no disponible",
            fuentes=[],
            busquedas_realizadas=False,
            tiempo_respuesta=0.0,
            modelo_usado="error"
        )

# Prompt integral para investigaci√≥n completa de empresa y puesto
def crear_prompt_integral(empresa: str, puesto: str) -> str:
    """Busca informaci√≥n completa sobre la empresa, cultura y puesto en una sola consulta"""
    return f"""Busca informaci√≥n COMPLETA y detallada sobre {empresa} y el puesto {puesto}. Incluye TODA la informaci√≥n necesaria para generar preguntas de entrevista contextualizadas:

INFORMACI√ìN EMPRESARIAL COMPLETA:
- Historia, fundaci√≥n y evoluci√≥n de {empresa}
- Modelo de negocio, servicios principales y productos
- Tama√±o de la empresa (empleados, facturaci√≥n, presencia global)
- Tecnolog√≠as, plataformas y herramientas espec√≠ficas que utiliza {empresa}
- Proyectos actuales, iniciativas importantes y desarrollos recientes
- Posici√≥n en el mercado, competidores y sector de la industria
- Noticias recientes y desarrollos estrat√©gicos (√∫ltimos 6-12 meses)

CULTURA ORGANIZACIONAL Y AMBIENTE LABORAL:
- Valores corporativos, principios y misi√≥n de {empresa}
- Metodolog√≠as de trabajo y procesos (√°gil, remoto, presencial, h√≠brido)
- Beneficios espec√≠ficos y pol√≠ticas de recursos humanos
- Programas de desarrollo profesional y crecimiento
- Estilo de liderazgo y estructura organizacional
- Ambiente de trabajo y cultura de equipo
- Testimonios de empleados y experiencias laborales

CONTEXTO ESPEC√çFICO DEL PUESTO {puesto}:
- Responsabilidades exactas del {puesto} en {empresa}
- Tecnolog√≠as, herramientas y metodolog√≠as espec√≠ficas para este rol
- Estructura del equipo y colaboraci√≥n interdepartamental
- Proyectos t√≠picos y retos espec√≠ficos del {puesto} en {empresa}
- Perfil ideal y competencias buscadas por {empresa}
- Oportunidades de crecimiento y desarrollo profesional
- Contexto salarial y compensaci√≥n en el mercado

FUENTES A CONSULTAR:
- P√°gina web oficial de {empresa}
- LinkedIn (empresa y empleados)
- Glassdoor y plataformas de rese√±as laborales
- Ofertas laborales actuales de {empresa}
- Comunicados de prensa y noticias recientes
- Entrevistas con ejecutivos y empleados
- Informes de industria y an√°lisis de mercado

Proporciona informaci√≥n detallada, espec√≠fica y verificable que permita generar preguntas de entrevista contextualizadas con datos reales de {empresa}."""

# Funci√≥n espec√≠fica para b√∫squeda de informaci√≥n de empresa
def crear_prompt_empresa(empresa: str, puesto: str) -> str:
    """Busca informaci√≥n espec√≠fica sobre la empresa en Per√∫ - contexto, tecnolog√≠as, cultura"""
    return f"""Busca informaci√≥n DETALLADA y espec√≠fica sobre {empresa} en Per√∫, enfoc√°ndote en el contexto para entrevistas del puesto {puesto}:

INFORMACI√ìN EMPRESARIAL EN PER√ö:
- Historia y presencia de {empresa} en Per√∫ espec√≠ficamente
- Operaciones, servicios y productos de {empresa} en el mercado peruano
- Tama√±o de operaciones en Per√∫ (empleados, oficinas, proyectos)
- Tecnolog√≠as, plataformas y herramientas espec√≠ficas que utiliza {empresa} en Per√∫
- Stack tecnol√≥gico, metodolog√≠as de desarrollo y arquitectura t√©cnica
- Proyectos actuales y desarrollos importantes en Per√∫
- Clientes principales y sectores que atiende en Per√∫

CULTURA Y AMBIENTE LABORAL EN PER√ö:
- Valores corporativos y cultura organizacional de {empresa} en Per√∫
- Metodolog√≠as de trabajo (√°gil, DevOps, frameworks espec√≠ficos)
- Modalidad de trabajo (remoto, h√≠brido, presencial) en oficinas peruanas
- Beneficios y pol√≠ticas espec√≠ficas para empleados en Per√∫
- Programas de capacitaci√≥n y desarrollo profesional
- Ambiente de trabajo y testimonios de empleados en Per√∫

CONTEXTO ESPEC√çFICO DEL PUESTO {puesto}:
- C√≥mo opera el √°rea de {puesto} dentro de {empresa} en Per√∫
- Tecnolog√≠as y herramientas espec√≠ficas para {puesto} en {empresa}
- Proyectos t√≠picos y responsabilidades del {puesto} en {empresa}
- Perfil buscado y competencias valoradas por {empresa} para {puesto}

NOTICIAS Y DESARROLLOS RECIENTES:
- Noticias recientes de {empresa} en Per√∫ (√∫ltimos 6-12 meses)
- Expansiones, nuevos proyectos o iniciativas en Per√∫
- Comunicados de prensa y desarrollos estrat√©gicos

Enf√≥cate en informaci√≥n verificable y espec√≠fica que permita generar preguntas contextualizadas para una entrevista de {puesto} en {empresa}."""

# Funci√≥n espec√≠fica para b√∫squeda de mercado laboral del puesto
def crear_prompt_puesto_mercado(puesto: str) -> str:
    """Busca informaci√≥n sobre puestos similares en otras empresas de Per√∫ para contexto de mercado"""
    return f"""Busca informaci√≥n sobre el mercado laboral del puesto {puesto} en Per√∫, incluyendo otras empresas y contexto sectorial:

AN√ÅLISIS DE MERCADO DEL PUESTO {puesto} EN PER√ö:
- Principales empresas en Per√∫ que contratan para {puesto}
- Tecnolog√≠as m√°s demandadas para {puesto} en el mercado peruano
- Habilidades y competencias m√°s valoradas en Per√∫ para {puesto}
- Rangos salariales y compensaci√≥n t√≠pica para {puesto} en Per√∫
- Tendencias actuales del mercado laboral para {puesto}

EMPRESAS REFERENCIALES EN PER√ö:
- Principales empresas tecnol√≥gicas/consultoras que contratan {puesto}
- Startups y empresas emergentes con posiciones de {puesto}
- Corporaciones multinacionales con operaciones en Per√∫
- Modalidades de trabajo m√°s comunes (remoto, h√≠brido, presencial)

COMPETENCIAS Y TECNOLOG√çAS EN DEMANDA:
- Stack tecnol√≥gico m√°s solicitado para {puesto} en Per√∫
- Certificaciones y habilidades t√©cnicas valoradas
- Soft skills y competencias blandas importantes
- Metodolog√≠as y frameworks m√°s utilizados

CONTEXTO SECTORIAL:
- Sectores de la industria que m√°s demandan {puesto} en Per√∫
- Proyectos t√≠picos y retos comunes en {puesto}
- Oportunidades de crecimiento profesional en el mercado peruano
- Tendencias de transformaci√≥n digital que afectan {puesto}

Esta informaci√≥n ayudar√° a entender el contexto competitivo y las expectativas del mercado para {puesto} en Per√∫."""

# Funci√≥n espec√≠fica para b√∫squeda de informaci√≥n personal del entrevistador
def crear_prompt_entrevistador_personal(nombre_entrevistador: str, empresa: str) -> str:
    """Busca informaci√≥n general sobre el entrevistador para establecer conexi√≥n"""
    return f"""¬øQu√© sabes de {nombre_entrevistador}? Busca en internet toda la informaci√≥n posible sobre esta persona de cualquier tipo.

Busca informaci√≥n general y completa sobre {nombre_entrevistador}:

- Cualquier informaci√≥n personal, profesional o p√∫blica disponible
- Perfil en redes sociales (LinkedIn, Twitter, Instagram, etc.)
- Art√≠culos, publicaciones, entrevistas o contenido que haya creado
- Participaci√≥n en eventos, conferencias o charlas
- Educaci√≥n, experiencia laboral y trayectoria
- Intereses, hobbies o actividades personales
- Proyectos, logros o reconocimientos
- Personalidad, estilo de comunicaci√≥n y valores
- Cualquier otra informaci√≥n relevante que encuentres

Proporciona toda la informaci√≥n disponible sobre {nombre_entrevistador} para entender su personalidad y estilo como entrevistador."""

# Funci√≥n para generar preguntas con OpenAI
def generar_preguntas(propuesta: PropuestaLaboral, informacion_integral: str) -> dict:
    """Genera preguntas usando OpenAI con contexto integral enriquecido"""
    
    # Usar la informaci√≥n integral obtenida de la b√∫squeda √∫nica
    info_completa = f"""
INFORMACI√ìN INTEGRAL INVESTIGADA SOBRE {propuesta.empresa}:
{informacion_integral}
"""
    
    prompt = f"""
    Eres un experto en recursos humanos y entrevistas t√©cnicas especializadas.
    
    PROPUESTA LABORAL:
    - Empresa: {propuesta.empresa}
    - Puesto: {propuesta.puesto}
    - Descripci√≥n: {propuesta.descripcion}
    - Requisitos: {propuesta.requisitos}
    
    CONTEXTO ESPEC√çFICO INVESTIGADO:
    {info_completa}
    
    INSTRUCCIONES ESPEC√çFICAS:
    Bas√°ndote DIRECTAMENTE en la informaci√≥n espec√≠fica investigada sobre {propuesta.empresa}, genera:
    
    10-12 preguntas de EVALUACI√ìN CONTEXTUALIZADAS para que el ENTREVISTADOR le haga al CANDIDATO:
    
    TIPOS DE PREGUNTAS CON CONTEXTO ESPEC√çFICO:
    - 3-4 preguntas t√©cnicas que mencionen EXPL√çCITAMENTE las tecnolog√≠as, herramientas, plataformas o metodolog√≠as espec√≠ficas que usa {propuesta.empresa} seg√∫n la investigaci√≥n
    - 3-4 preguntas situacionales que incorporen DIRECTAMENTE los valores, cultura, metodolog√≠as de trabajo o retos espec√≠ficos mencionados en la investigaci√≥n sobre {propuesta.empresa}
    - 4-5 preguntas de competencias que incluyan REFERENCIAS ESPEC√çFICAS a proyectos reales, productos, servicios o contexto de mercado de {propuesta.empresa} encontrado en la investigaci√≥n
    
    C√ìMO POTENCIAR LAS PREGUNTAS CON CONTEXTO:
    - INCLUYE nombres espec√≠ficos de tecnolog√≠as, productos, plataformas que usa {propuesta.empresa}
    - MENCIONA metodolog√≠as, procesos o enfoques espec√≠ficos encontrados en la investigaci√≥n
    - INCORPORA retos reales, proyectos actuales o iniciativas espec√≠ficas de {propuesta.empresa}
    - REFERENCIA la cultura, valores o ambiente de trabajo espec√≠fico identificado
    - USA el contexto de la industria, mercado o sector donde opera {propuesta.empresa}
    
    FORMATO DE PREGUNTAS CONTEXTUALIZADAS:
    - "En {propuesta.empresa} utilizamos [tecnolog√≠a espec√≠fica encontrada] para [contexto espec√≠fico]. ¬øC√≥mo abordar√≠as [situaci√≥n t√©cnica]?"
    - "Considerando que {propuesta.empresa} [contexto cultural/organizacional espec√≠fico], describe c√≥mo manejar√≠as [situaci√≥n]"
    - "Dado que {propuesta.empresa} est√° trabajando en [proyecto/iniciativa espec√≠fica encontrada], ¬øqu√© estrategia usar√≠as para [competencia espec√≠fica]?"
    
    REQUISITOS CR√çTICOS:
    - CADA pregunta DEBE incluir al menos UNA referencia espec√≠fica de la investigaci√≥n
    - EVITA preguntas gen√©ricas - todas deben estar contextualizadas con informaci√≥n real de {propuesta.empresa}
    - INCORPORA datos concretos, nombres de productos, tecnolog√≠as, proyectos o metodolog√≠as encontradas
    - Las preguntas eval√∫an al candidato USANDO el contexto espec√≠fico como marco de referencia
    
    Responde √öNICAMENTE en formato JSON v√°lido con esta estructura exacta:
    {{
        "preguntas": [
            "texto de pregunta 1 como string",
            "texto de pregunta 2 como string"
        ]
    }}
    
    IMPORTANTE: Las preguntas deben ser strings directos, NO objetos con llaves.
    """
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Eres un experto en RRHH que genera preguntas de entrevista CONTEXTUALIZADAS y espec√≠ficas. SIEMPRE incluyes informaci√≥n espec√≠fica de la empresa investigada en cada pregunta. Las preguntas deben incorporar tecnolog√≠as, proyectos, cultura y contexto real de la empresa. Responde SIEMPRE en formato JSON v√°lido con arrays de strings, NUNCA objetos con llaves."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            response_format={"type": "json_object"}
        )
        
        import json
        resultado = json.loads(response.choices[0].message.content)
        
        # Procesar preguntas para asegurar formato correcto
        preguntas_raw = resultado.get('preguntas', [])
        preguntas = []
        
        for pregunta in preguntas_raw:
            if isinstance(pregunta, dict):
                # Si OpenAI devolvi√≥ un objeto, extraer el texto de la pregunta
                texto_pregunta = pregunta.get('pregunta', str(pregunta))
                preguntas.append(texto_pregunta)
            elif isinstance(pregunta, str):
                # Si ya es string, usarlo directamente
                preguntas.append(pregunta)
            else:
                # Convertir a string como fallback
                preguntas.append(str(pregunta))
        
        print(f"‚úÖ Preguntas contextualizadas generadas: {len(preguntas)} preguntas potenciadas con informaci√≥n espec√≠fica de {propuesta.empresa}")
        
        if preguntas:
            print(f"\nü§î PREGUNTAS CONTEXTUALIZADAS CON INFORMACI√ìN ESPEC√çFICA DE {propuesta.empresa.upper()}:")
            print(f"{'='*80}")
            for i, pregunta in enumerate(preguntas, 1):
                print(f"  {i}. {pregunta}")
                print()
            print(f"{'='*80}")
        
        # Devolver resultado con formato corregido
        return {"preguntas": preguntas}
    except Exception as e:
        print(f"‚ùå Error en OpenAI: {e}")
        return {"preguntas": []}

# Funci√≥n para generar preguntas con informaci√≥n m√∫ltiple (empresa + mercado + entrevistador)
def generar_preguntas_con_contexto_multiple(propuesta: PropuestaLaboral, info_empresa: str = "", info_mercado: str = "", info_entrevistador: str = "") -> dict:
    """Genera preguntas usando OpenAI con m√∫ltiples contextos de informaci√≥n"""
    
    # Construir informaci√≥n contextual combinada
    contexto_completo = ""
    
    if info_empresa:
        contexto_completo += f"""
INFORMACI√ìN ESPEC√çFICA DE {propuesta.empresa}:
{info_empresa}

"""
    
    if info_mercado:
        contexto_completo += f"""
CONTEXTO DEL MERCADO LABORAL PARA {propuesta.puesto} EN PER√ö:
{info_mercado}

"""
    
    if info_entrevistador:
        contexto_completo += f"""
INFORMACI√ìN PERSONAL DEL ENTREVISTADOR:
{info_entrevistador}

"""
    
    prompt = f"""
    Eres un experto en recursos humanos y entrevistas t√©cnicas especializadas.
    
    PROPUESTA LABORAL:
    - Empresa: {propuesta.empresa}
    - Puesto: {propuesta.puesto}
    - Descripci√≥n: {propuesta.descripcion}
    - Requisitos: {propuesta.requisitos}
    
    CONTEXTO INVESTIGADO:
    {contexto_completo}
    
    INSTRUCCIONES ESPEC√çFICAS:
    Genera 10-12 preguntas de EVALUACI√ìN CONTEXTUALIZADAS bas√°ndote PRINCIPALMENTE en la informaci√≥n de la EMPRESA y el MERCADO LABORAL:
    
    FUENTES PRINCIPALES PARA LAS PREGUNTAS:
    - Informaci√≥n espec√≠fica de {propuesta.empresa}: tecnolog√≠as, cultura, proyectos, metodolog√≠as
    - An√°lisis del mercado laboral para {propuesta.puesto}: tendencias, competencias demandadas, tecnolog√≠as populares
    
    TIPOS DE PREGUNTAS:
    - 4-5 preguntas t√©cnicas incorporando tecnolog√≠as espec√≠ficas de {propuesta.empresa} y tecnolog√≠as demandadas en el mercado
    - 3-4 preguntas situacionales basadas en la cultura y metodolog√≠as de {propuesta.empresa}
    - 3-4 preguntas de competencias que combinen los retos espec√≠ficos de {propuesta.empresa} con las competencias valoradas en el mercado
    
    C√ìMO ESTRUCTURAR LAS PREGUNTAS:
    - PRIORIZA informaci√≥n espec√≠fica de {propuesta.empresa} (tecnolog√≠as, proyectos, cultura, metodolog√≠as)
    - INCORPORA tendencias y competencias del mercado laboral para {propuesta.puesto} en Per√∫
    - MENCIONA herramientas, frameworks y tecnolog√≠as espec√≠ficas encontradas
    - REFERENCIA proyectos actuales, retos reales y contexto espec√≠fico de {propuesta.empresa}
    
    USO DE LA INFORMACI√ìN DEL ENTREVISTADOR:
    La informaci√≥n del entrevistador es SOLO para contexto y consejos de conexi√≥n personal, NO para generar las preguntas principales.
    - Si hay informaci√≥n del entrevistador, √∫sala para generar consejos de conexi√≥n
    - Adapta el tono/estilo de las preguntas seg√∫n la personalidad del entrevistador
    - Sugiere temas de conversaci√≥n casual basados en sus intereses
    
    REQUISITOS:
    - Las preguntas deben evaluar competencias t√©cnicas y profesionales espec√≠ficas
    - Basar TODAS las preguntas en informaci√≥n concreta de empresa + mercado
    - Evita preguntas gen√©ricas o que no tengan contexto espec√≠fico
    - Genera consejos separados para establecer conexi√≥n personal con el entrevistador
    
    Responde √öNICAMENTE en formato JSON v√°lido con esta estructura exacta:
    {{
        "preguntas": [
            "texto de pregunta 1 como string",
            "texto de pregunta 2 como string"
        ],
        "consejos_conexion": [
            "consejo 1 como string",
            "consejo 2 como string"
        ]
    }}
    
    IMPORTANTE: Las preguntas deben ser strings directos, NO objetos con llaves.
    """
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Eres un experto en RRHH que genera preguntas contextualizadas combinando informaci√≥n de empresa, mercado y entrevistador. Creas conexiones humanas aut√©nticas. Responde SIEMPRE en formato JSON v√°lido con arrays de strings, NUNCA objetos con llaves."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            response_format={"type": "json_object"}
        )
        
        import json
        resultado = json.loads(response.choices[0].message.content)
        
        # Procesar preguntas para asegurar formato correcto
        preguntas_raw = resultado.get('preguntas', [])
        preguntas = []
        
        for pregunta in preguntas_raw:
            if isinstance(pregunta, dict):
                # Si OpenAI devolvi√≥ un objeto, extraer el texto de la pregunta
                texto_pregunta = pregunta.get('pregunta', str(pregunta))
                preguntas.append(texto_pregunta)
            elif isinstance(pregunta, str):
                # Si ya es string, usarlo directamente
                preguntas.append(pregunta)
            else:
                # Convertir a string como fallback
                preguntas.append(str(pregunta))
        
        # Procesar consejos de conexi√≥n
        consejos_raw = resultado.get('consejos_conexion', [])
        consejos = []
        
        for consejo in consejos_raw:
            if isinstance(consejo, dict):
                # Si es objeto, extraer el texto
                texto_consejo = consejo.get('consejo', str(consejo))
                consejos.append(texto_consejo)
            elif isinstance(consejo, str):
                # Si ya es string, usarlo directamente
                consejos.append(consejo)
            else:
                # Convertir a string como fallback
                consejos.append(str(consejo))
        
        print(f"‚úÖ Preguntas contextualizadas generadas: {len(preguntas)} preguntas con contexto m√∫ltiple")
        if consejos:
            print(f"‚úÖ Consejos de conexi√≥n personal generados: {len(consejos)} consejos")
        
        # Devolver resultado con formato corregido
        return {
            "preguntas": preguntas,
            "consejos_conexion": consejos
        }
    except Exception as e:
        print(f"‚ùå Error en OpenAI: {e}")
        return {"preguntas": [], "consejos_conexion": []}

@app.get("/")
async def root():
    return {
        "mensaje": "API de Entrevistas - Entre-Vistas", 
        "version": "3.1", 
        "funcionalidades": [
            "üöÄ SIEMPRE M√ÅXIMA CALIDAD (sonar-pro)",
            "1 B√∫squeda integral ultra-completa",
            "M√°ximo 3 fuentes de alta calidad",
            "2500 tokens para m√°ximo detalle en una sola consulta",
            "Informaci√≥n empresarial + cultura + puesto en b√∫squeda unificada", 
            "Contexto completo e integrado de empresa y rol",
            "10-12 preguntas potenciadas con contexto integral espec√≠fico",
            "Referencias verificadas y de alta calidad",
            "Eficiencia optimizada con una sola consulta API"
        ],
        "configuracion_maxima_calidad": {
            "modelo": "sonar-pro",
            "tokens": 2500,
            "temperature": 0.1,
            "profundidad": "advanced",
            "fuentes_maximas": 3
        },
        "busqueda_integral": {
            "tipo": "B√∫squeda completa unificada",
            "incluye": "Empresa + Cultura + Puesto + Contexto espec√≠fico",
            "fuentes": "M√°ximo 3 fuentes de alta calidad",
            "eficiencia": "Una sola consulta API optimizada"
        },
        "metricas_calidad": {
            "Alta": "3 fuentes encontradas",
            "Media": "2 fuentes encontradas", 
            "Baja": "1 fuente encontrada"
        }
    }

@app.post("/test-sonar")
async def test_sonar(data: dict):
    """Endpoint de debug para verificar la b√∫squeda integral de Sonar con M√ÅXIMA CALIDAD"""
    empresa = data.get("empresa", "Entel Per√∫")
    puesto = data.get("puesto", "IA Engineer")
    
    try:
        # B√∫squeda integral unificada
        query = crear_prompt_integral(empresa, puesto)
        resultado = buscar_con_sonar(query)
        
        return {
            "tipo_busqueda": "integral_completa",
            "empresa": empresa,
            "puesto": puesto,
            "query_enviado": query,
            "resultado": {
                "contenido": resultado.contenido,
                "fuentes": resultado.fuentes,
                "busquedas_web_realizadas": resultado.busquedas_realizadas,
                "tiempo_respuesta_segundos": resultado.tiempo_respuesta,
                "modelo_usado": resultado.modelo_usado,
                "numero_fuentes": len(resultado.fuentes)
            },
            "verificacion_web": {
                "tiene_fuentes": len(resultado.fuentes) > 0,
                "busquedas_confirmadas": resultado.busquedas_realizadas,
                "fuentes_obtenidas": f"{len(resultado.fuentes)}/3",
                "calidad_busqueda": "Alta" if len(resultado.fuentes) >= 3 else "Media" if len(resultado.fuentes) >= 2 else "Baja",
                "tipo": "B√∫squeda Integral (Empresa + Cultura + Puesto)"
            }
        }
        
    except Exception as e:
        return {"error": str(e)}

@app.post("/generar-entrevista", response_model=RespuestaEntrevista)
async def generar_entrevista(propuesta_texto: PropuestaLaboralTexto):
    """Endpoint principal para generar preguntas de entrevista desde texto libre con investigaci√≥n web integral"""
    
    print(f"üìù Texto recibido: {propuesta_texto.texto[:100]}...")
    
    try:
        # 1. Extraer informaci√≥n estructurada del texto con OpenAI
        propuesta = extraer_informacion_propuesta(propuesta_texto.texto)
        
        print("\nüìã PROPUESTA LABORAL EXTRA√çDA:")
        print(f"{'='*80}")
        print(f"üè¢ Empresa: {propuesta.empresa}")
        print(f"üíº Puesto: {propuesta.puesto}")
        print(f"üìÑ Descripci√≥n: {propuesta.descripcion[:200]}...")
        print(f"‚ö° Requisitos: {propuesta.requisitos[:200]}...")
        print(f"{'='*80}")
        print(f"‚úÖ Propuesta extra√≠da: {propuesta.empresa} - {propuesta.puesto}")
        
        # 2. B√∫squeda integral con Sonar - UNA SOLA b√∫squeda completa con M√ÅXIMA CALIDAD
        print(f"\n{'='*80}")
        print("üöÄ INICIANDO B√öSQUEDA INTEGRAL COMPLETA CON SONAR (M√ÅXIMA CALIDAD)")
        print("üîß Configuraci√≥n: sonar-pro | 2500 tokens | temp=0.1 | profundidad=advanced")
        print(f"{'='*80}")
        
        # B√öSQUEDA INTEGRAL: Informaci√≥n completa de empresa, cultura y puesto (M√ÅXIMA CALIDAD)
        print(f"\nüîç B√öSQUEDA INTEGRAL: {propuesta.empresa} + {propuesta.puesto}")
        print(f"{'-'*80}")
        query_integral = crear_prompt_integral(propuesta.empresa, propuesta.puesto)
        info_integral = buscar_con_sonar(query_integral)
        
        # 3. Preparar informaci√≥n integral para generaci√≥n de preguntas
        informacion_completa = info_integral.contenido
        total_fuentes = len(info_integral.fuentes)
        tiempo_total = info_integral.tiempo_respuesta
        
        print("\nüìä RESUMEN DE LA B√öSQUEDA INTEGRAL (M√ÅXIMA CALIDAD):")
        print(f"{'='*80}")
        print(f"üîç B√∫squeda Integral Completa ({total_fuentes}/3 fuentes): {tiempo_total:.2f}s - {info_integral.modelo_usado}")
        print("üöÄ Configuraci√≥n: sonar-pro, 2500 tokens, temp=0.1")
        print("üìö Informaci√≥n obtenida: Empresa + Cultura + Puesto en una sola consulta")
        print(f"üìö Total: {total_fuentes} fuentes espec√≠ficas en {tiempo_total:.2f}s")
        print(f"{'='*80}")
        print(f"üìö Investigaci√≥n integral completa con M√ÅXIMA CALIDAD: {total_fuentes} fuentes")
        
        # 4. Generar preguntas contextualizadas con OpenAI
        print(f"\n{'='*80}")
        print("üí° GENERANDO PREGUNTAS CONTEXTUALIZADAS CON INFORMACI√ìN INTEGRAL DE LA EMPRESA")
        print(f"{'='*80}")
        resultado = generar_preguntas(propuesta, informacion_completa)
        
        # 5. Extraer preguntas del resultado
        preguntas = resultado.get("preguntas", [])
        
        # 6. Construir respuesta completa
        respuesta = RespuestaEntrevista(
            preguntas=preguntas,
            informacion_empresa={
                "nombre": propuesta.empresa,
                "informacion_encontrada": info_integral.contenido[:800] + "..." if len(info_integral.contenido) > 800 else info_integral.contenido,
                "fuentes_consultadas": len(info_integral.fuentes),
                "busquedas_web_verificadas": info_integral.busquedas_realizadas
            },
            propuesta_extraida={
                "empresa": propuesta.empresa,
                "puesto": propuesta.puesto,
                "descripcion": propuesta.descripcion,
                "requisitos": propuesta.requisitos
            },
            investigacion_detallada={
                "resumen_fuentes": {
                    "busqueda_integral": {"total": len(info_integral.fuentes), "modelo": info_integral.modelo_usado}
                },
                "calidad_investigacion": "Alta" if total_fuentes >= 3 else "Media" if total_fuentes >= 2 else "Baja",
                "busqueda_web_realizada": info_integral.busquedas_realizadas,
                "tiempo_total": tiempo_total,
                "tipo_busqueda": "B√∫squeda Integral Completa (Empresa + Cultura + Puesto)"
            }
        )
        
        print(f"\n{'='*80}")
        print("üéâ PROCESO COMPLETADO EXITOSAMENTE CON M√ÅXIMA CALIDAD")
        print(f"{'='*80}")
        print("üìä Resultados finales:")
        print(f"   ‚Ä¢ {len(preguntas)} preguntas contextualizadas con informaci√≥n integral de la empresa")
        print(f"   ‚Ä¢ {total_fuentes}/3 fuentes de alta calidad consultadas en una sola b√∫squeda")
        print(f"   ‚Ä¢ Tiempo total: {tiempo_total:.2f} segundos")
        print("   ‚Ä¢ Configuraci√≥n: sonar-pro, 2500 tokens en b√∫squeda integral")
        print(f"   ‚Ä¢ Calidad investigaci√≥n: {'Alta' if total_fuentes >= 3 else 'Media' if total_fuentes >= 2 else 'Baja'}")
        print(f"{'='*80}")
        print(f"üöÄ Respuesta construida con M√ÅXIMA CALIDAD: {len(preguntas)} preguntas, {total_fuentes} fuentes")
        return respuesta
        
    except Exception as e:
        print(f"‚ùå Error en generar_entrevista: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error procesando la solicitud: {str(e)}")

@app.post("/generar-entrevista-con-opciones", response_model=RespuestaEntrevista)
async def generar_entrevista_con_opciones(propuesta_opciones: PropuestaLaboralConOpciones):
    """Endpoint principal con b√∫squedas opcionales (empresa, mercado, reclutador) usando m√°xima calidad"""
    
    print(f"üìù Texto recibido: {propuesta_opciones.texto[:100]}...")
    print("üîç Opciones de b√∫squeda:")
    print(f"   ‚Ä¢ Buscar empresa: {propuesta_opciones.buscar_empresa}")
    print(f"   ‚Ä¢ Buscar mercado: {propuesta_opciones.buscar_puesto_mercado}")
    print(f"   ‚Ä¢ Buscar entrevistador: {propuesta_opciones.buscar_entrevistador}")
    if propuesta_opciones.nombre_entrevistador:
        print(f"   ‚Ä¢ Nombre entrevistador: {propuesta_opciones.nombre_entrevistador}")
    
    try:
        # 1. Extraer informaci√≥n estructurada del texto con OpenAI
        propuesta = extraer_informacion_propuesta(propuesta_opciones.texto)
        
        print("\nüìã PROPUESTA LABORAL EXTRA√çDA:")
        print(f"{'='*80}")
        print(f"üè¢ Empresa: {propuesta.empresa}")
        print(f"üíº Puesto: {propuesta.puesto}")
        print(f"üìÑ Descripci√≥n: {propuesta.descripcion[:200]}...")
        print(f"‚ö° Requisitos: {propuesta.requisitos[:200]}...")
        print(f"{'='*80}")
        
        # 2. Preparar b√∫squedas opcionales para paralelizaci√≥n
        busquedas_pendientes = []
        info_empresa = ""
        info_mercado = ""
        info_entrevistador = ""
        
        print(f"\n{'='*80}")
        print("üöÄ INICIANDO B√öSQUEDAS OPCIONALES PARALELAS CON M√ÅXIMA CALIDAD")
        print("üîß Configuraci√≥n: sonar-pro | 2500 tokens | temp=0.1 | profundidad=advanced")
        print(f"{'='*80}")
        
        # Preparar b√∫squedas seg√∫n configuraci√≥n
        if propuesta_opciones.buscar_empresa:
            query_empresa = crear_prompt_empresa(propuesta.empresa, propuesta.puesto)
            busquedas_pendientes.append((query_empresa, f"EMPRESA ({propuesta.empresa})"))
            
        if propuesta_opciones.buscar_puesto_mercado:
            query_mercado = crear_prompt_puesto_mercado(propuesta.puesto)
            busquedas_pendientes.append((query_mercado, f"MERCADO ({propuesta.puesto})"))
            
        if propuesta_opciones.buscar_entrevistador and propuesta_opciones.nombre_entrevistador:
            query_entrevistador = crear_prompt_entrevistador_personal(propuesta_opciones.nombre_entrevistador, propuesta.empresa)
            busquedas_pendientes.append((query_entrevistador, f"ENTREVISTADOR ({propuesta_opciones.nombre_entrevistador})"))
        elif propuesta_opciones.buscar_entrevistador and not propuesta_opciones.nombre_entrevistador:
            print("\n‚ö†Ô∏è  B√öSQUEDA ENTREVISTADOR ACTIVADA PERO SIN NOMBRE - OMITIDA")
        
        # Ejecutar b√∫squedas en paralelo
        resultados = []
        busquedas_realizadas = []
        total_fuentes = 0
        
        if busquedas_pendientes:
            resultados = await ejecutar_busquedas_paralelas(busquedas_pendientes)
            
            # Procesar resultados y asignar informaci√≥n
            for i, (query, nombre) in enumerate(busquedas_pendientes):
                resultado = resultados[i]
                
                if "EMPRESA" in nombre:
                    info_empresa = resultado.contenido
                    busquedas_realizadas.append({
                        "tipo": "empresa",
                        "fuentes": len(resultado.fuentes),
                        "tiempo": resultado.tiempo_respuesta,
                        "modelo": resultado.modelo_usado
                    })
                elif "MERCADO" in nombre:
                    info_mercado = resultado.contenido
                    busquedas_realizadas.append({
                        "tipo": "mercado",
                        "fuentes": len(resultado.fuentes),
                        "tiempo": resultado.tiempo_respuesta,
                        "modelo": resultado.modelo_usado
                    })
                elif "ENTREVISTADOR" in nombre:
                    info_entrevistador = resultado.contenido
                    busquedas_realizadas.append({
                        "tipo": "entrevistador",
                        "fuentes": len(resultado.fuentes),
                        "tiempo": resultado.tiempo_respuesta,
                        "modelo": resultado.modelo_usado
                    })
                
                total_fuentes += len(resultado.fuentes)
        
        # Calcular tiempo total (el tiempo real de la paralelizaci√≥n)
        tiempo_total = max([b["tiempo"] for b in busquedas_realizadas]) if busquedas_realizadas else 0.0
        
        # 3. Resumen de b√∫squedas realizadas
        print("\nüìä RESUMEN DE B√öSQUEDAS PARALELAS (M√ÅXIMA CALIDAD):")
        print(f"{'='*80}")
        tiempo_individual_total = 0.0
        for busqueda in busquedas_realizadas:
            print(f"üîç {busqueda['tipo'].title()}: {busqueda['fuentes']}/3 fuentes - {busqueda['tiempo']:.2f}s - {busqueda['modelo']}")
            tiempo_individual_total += busqueda['tiempo']
        
        if busquedas_realizadas:
            ahorro_tiempo = tiempo_individual_total - tiempo_total
            porcentaje_ahorro = (ahorro_tiempo / tiempo_individual_total * 100) if tiempo_individual_total > 0 else 0
            print(f"üìö Total: {total_fuentes} fuentes en {tiempo_total:.2f}s ({len(busquedas_realizadas)} b√∫squedas PARALELAS)")
            print(f"‚ö° Ahorro de tiempo: {ahorro_tiempo:.2f}s ({porcentaje_ahorro:.1f}% m√°s r√°pido que secuencial)")
        print(f"{'='*80}")
        
        # 4. Generar preguntas contextualizadas con m√∫ltiple informaci√≥n
        print(f"\n{'='*80}")
        print("üí° GENERANDO PREGUNTAS CONTEXTUALIZADAS CON INFORMACI√ìN COMBINADA")
        print(f"{'='*80}")
        resultado = generar_preguntas_con_contexto_multiple(
            propuesta, 
            info_empresa, 
            info_mercado, 
            info_entrevistador
        )
        
        # 5. Extraer preguntas y consejos del resultado
        preguntas = resultado.get("preguntas", [])
        consejos_conexion = resultado.get("consejos_conexion", [])
        
        # 6. Construir respuesta completa
        respuesta = RespuestaEntrevista(
            preguntas=preguntas,
            consejos_conexion=consejos_conexion,
            informacion_empresa={
                "nombre": propuesta.empresa,
                "informacion_encontrada": info_empresa[:800] + "..." if len(info_empresa) > 800 else info_empresa,
                "fuentes_consultadas": len([b for b in busquedas_realizadas if b["tipo"] == "empresa"]),
                "busquedas_web_verificadas": any(b["fuentes"] > 0 for b in busquedas_realizadas)
            },
            propuesta_extraida={
                "empresa": propuesta.empresa,
                "puesto": propuesta.puesto,
                "descripcion": propuesta.descripcion,
                "requisitos": propuesta.requisitos
            },
            investigacion_detallada={
                "busquedas_opcionales": {
                    "empresa": {"activada": propuesta_opciones.buscar_empresa, "fuentes": len([b for b in busquedas_realizadas if b["tipo"] == "empresa"])},
                    "mercado": {"activada": propuesta_opciones.buscar_puesto_mercado, "fuentes": len([b for b in busquedas_realizadas if b["tipo"] == "mercado"])},
                    "entrevistador": {"activada": propuesta_opciones.buscar_entrevistador, "fuentes": len([b for b in busquedas_realizadas if b["tipo"] == "entrevistador"])}
                },
                "calidad_investigacion": "Alta" if total_fuentes >= 6 else "Media" if total_fuentes >= 3 else "Baja",
                "busqueda_web_realizada": len(busquedas_realizadas) > 0,
                "tiempo_total": tiempo_total,
                "total_fuentes": total_fuentes,
                "busquedas_completadas": len(busquedas_realizadas),
                "resultados_busquedas": {
                    "empresa": {
                        "contenido": info_empresa[:1000] + "..." if len(info_empresa) > 1000 else info_empresa,
                        "activada": propuesta_opciones.buscar_empresa,
                        "fuentes": len([b for b in busquedas_realizadas if b["tipo"] == "empresa"]),
                        "tiempo": next((b["tiempo"] for b in busquedas_realizadas if b["tipo"] == "empresa"), 0.0)
                    },
                    "mercado": {
                        "contenido": info_mercado[:1000] + "..." if len(info_mercado) > 1000 else info_mercado,
                        "activada": propuesta_opciones.buscar_puesto_mercado,
                        "fuentes": len([b for b in busquedas_realizadas if b["tipo"] == "mercado"]),
                        "tiempo": next((b["tiempo"] for b in busquedas_realizadas if b["tipo"] == "mercado"), 0.0)
                    },
                    "entrevistador": {
                        "contenido": info_entrevistador[:1000] + "..." if len(info_entrevistador) > 1000 else info_entrevistador,
                        "activada": propuesta_opciones.buscar_entrevistador,
                        "fuentes": len([b for b in busquedas_realizadas if b["tipo"] == "entrevistador"]),
                        "tiempo": next((b["tiempo"] for b in busquedas_realizadas if b["tipo"] == "entrevistador"), 0.0)
                    }
                }
            }
        )
        
        print(f"\n{'='*80}")
        print("üéâ PROCESO COMPLETADO EXITOSAMENTE CON B√öSQUEDAS PARALELAS")
        print(f"{'='*80}")
        print("üìä Resultados finales:")
        print(f"   ‚Ä¢ {len(preguntas)} preguntas contextualizadas con informaci√≥n combinada")
        print(f"   ‚Ä¢ {len(consejos_conexion)} consejos de conexi√≥n personal generados")
        print(f"   ‚Ä¢ {total_fuentes} fuentes de alta calidad consultadas en {len(busquedas_realizadas)} b√∫squedas PARALELAS")
        print(f"   ‚Ä¢ Tiempo total: {tiempo_total:.2f} segundos (paralelizaci√≥n optimizada)")
        print("   ‚Ä¢ Configuraci√≥n: sonar-pro, 2500 tokens por b√∫squeda")
        print(f"   ‚Ä¢ Calidad investigaci√≥n: {'Alta' if total_fuentes >= 6 else 'Media' if total_fuentes >= 3 else 'Baja'}")
        print(f"{'='*80}")
        
        return respuesta
        
    except Exception as e:
        print(f"‚ùå Error en generar_entrevista_con_opciones: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error procesando la solicitud: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000) 