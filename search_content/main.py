from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict
from openai import OpenAI
import requests
import os
from dotenv import load_dotenv
from datetime import datetime

# Cargar variables de entorno
load_dotenv()

app = FastAPI()

# Configurar CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Configurar clientes API
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
SONAR_API_KEY = os.getenv("SONAR_API_KEY")

# Modelos de datos
class PropuestaLaboralTexto(BaseModel):
    texto: str

class PropuestaLaboral(BaseModel):
    empresa: str
    puesto: str
    descripcion: str
    requisitos: str

class SonarResponse(BaseModel):
    contenido: str
    fuentes: List[Dict]
    busquedas_realizadas: bool
    tiempo_respuesta: float
    modelo_usado: str

class RespuestaEntrevista(BaseModel):
    preguntas: List[str]
    informacion_empresa: Dict
    propuesta_extraida: Dict
    investigacion_detallada: Dict

# FunciÃ³n para extraer informaciÃ³n de la propuesta
def extraer_informacion_propuesta(texto: str) -> PropuestaLaboral:
    """Extrae informaciÃ³n estructurada de un texto de propuesta laboral usando OpenAI"""
    
    prompt = f"""
    Analiza el siguiente texto de una propuesta laboral y extrae la informaciÃ³n estructurada.
    
    Texto de la propuesta:
    {texto}
    
    Extrae y responde en formato JSON con las siguientes claves:
    - "empresa": nombre de la empresa
    - "puesto": tÃ­tulo del puesto
    - "descripcion": descripciÃ³n del trabajo y responsabilidades
    - "requisitos": concatena toda la informaciÃ³n de requisitos, formaciÃ³n, experiencia y competencias en un solo texto
    
    Para el campo "requisitos", incluye toda la informaciÃ³n relevante en un pÃ¡rrafo continuo.
    """
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Eres un experto en anÃ¡lisis de propuestas laborales. Siempre responde en formato JSON vÃ¡lido con los campos exactos solicitados."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            response_format={"type": "json_object"}
        )
        
        import json
        datos = json.loads(response.choices[0].message.content)
        
        print(f"âœ… Datos extraÃ­dos: {datos}")
        
        return PropuestaLaboral(**datos)
    except Exception as e:
        print(f"âŒ Error extrayendo informaciÃ³n: {e}")
        return PropuestaLaboral(
            empresa="Empresa no identificada",
            puesto="Puesto no identificado", 
            descripcion=texto[:200],
            requisitos="No especificados"
        )

# FunciÃ³n optimizada para bÃºsquedas con Sonar - SIEMPRE MÃXIMA CALIDAD
def buscar_con_sonar(query: str, search_type: str = "pro") -> SonarResponse:
    """Busca informaciÃ³n usando la API de Sonar de Perplexity con MÃXIMA CALIDAD siempre"""
    
    start_time = datetime.now()
    url = "https://api.perplexity.ai/chat/completions"
    
    headers = {
        "Authorization": f"Bearer {SONAR_API_KEY}",
        "Content-Type": "application/json"
    }
    
    # ConfiguraciÃ³n SIEMPRE de mÃ¡xima calidad
    modelo = "sonar-pro"
    max_tokens = 2500  # Aumentado para mÃ¡xima calidad
    temperature = 0.1  # MÃ¡s preciso para bÃºsquedas especÃ­ficas
    
    payload = {
        "model": modelo,
        "messages": [
            {
                "role": "system",
                "content": "Eres un investigador experto de Ã©lite que proporciona informaciÃ³n actualizada, precisa y extremadamente detallada basada en fuentes web confiables de alta calidad. Prioriza fuentes oficiales, verificables y recientes. Incluye detalles especÃ­ficos, datos concretos y contexto relevante en cada respuesta."
            },
            {
                "role": "user", 
                "content": query
            }
        ],
        "max_tokens": max_tokens,
        "temperature": temperature,
        # "search_domain_filter": []  # Permitir bÃºsquedas en toda la web
        "search_recency_filter": "month",  # Ãšltimas bÃºsquedas del mes
        "return_related_questions": True,
        "search_depth_filter": "advanced",  # BÃºsqueda mÃ¡s profunda
        "enable_clarification_questions": True  # Preguntas de clarificaciÃ³n para mejor contexto
    }
    
    try:
        if not SONAR_API_KEY:
            print("âŒ SONAR_API_KEY no configurada")
            return SonarResponse(
                contenido="InformaciÃ³n no disponible - API key no configurada",
                fuentes=[],
                busquedas_realizadas=False,
                tiempo_respuesta=0.0,
                modelo_usado="none"
            )
            
        print(f"ğŸ” Iniciando bÃºsqueda con {modelo} (MÃXIMA CALIDAD)")
        print(f"ğŸ“ Prompt enviado: {query[:150]}...")
        print(f"âš¡ ConfiguraciÃ³n: {max_tokens} tokens, temp={temperature}")
        print("â±ï¸  Esperando respuesta...")
        
        response = requests.post(url, json=payload, headers=headers, timeout=45)
        tiempo_respuesta = (datetime.now() - start_time).total_seconds()
        
        if response.status_code == 200:
            data = response.json()
            print(f"ğŸ” Debug - Claves en respuesta: {list(data.keys())}")
            
            # Validar estructura de respuesta
            if "choices" not in data or not data["choices"]:
                print("âŒ Error: Respuesta sin campo 'choices'")
                return SonarResponse(
                    contenido="Error en estructura de respuesta",
                    fuentes=[],
                    busquedas_realizadas=False,
                    tiempo_respuesta=tiempo_respuesta,
                    modelo_usado=modelo
                )
            
            content = data["choices"][0]["message"]["content"]
            
            # Mostrar el contenido de la respuesta en consola
            print("ğŸ“„ CONTENIDO DE LA BÃšSQUEDA:")
            print(f"{'='*80}")
            print(content[:500] + "..." if len(content) > 500 else content)
            print(f"{'='*80}")
            
            # Extraer fuentes/citas de la respuesta
            fuentes = []
            
            # Debug: mostrar estructura de datos adicionales (limitado a 3 fuentes)
            if "search_results" in data:
                print(f"ğŸ” Debug - Encontrados {len(data['search_results'])} resultados (usando mÃ¡ximo 3)")
                if data["search_results"]:
                    print(f"ğŸ” Debug - Primer tipo de resultado: {type(data['search_results'][0])}")
            
            if "citations" in data:
                print(f"ğŸ” Debug - Encontradas {len(data['citations'])} citas (usando mÃ¡ximo 3)")
                if data["citations"]:
                    print(f"ğŸ” Debug - Primer tipo de cita: {type(data['citations'][0])}")
            
            # Extraer fuentes de search_results (limitado a 3 fuentes)
            if "search_results" in data and data["search_results"]:
                for i, result in enumerate(data["search_results"][:3]):  # MÃ¡ximo 3 fuentes
                    if isinstance(result, dict):
                        fuentes.append({
                            "numero": i + 1,
                            "titulo": result.get("title", result.get("name", "TÃ­tulo no disponible"))[:80] + "..." if len(result.get("title", result.get("name", ""))) > 80 else result.get("title", result.get("name", "TÃ­tulo no disponible")),
                            "url": result.get("url", "URL no disponible"),
                            "fecha": result.get("date", result.get("published_date", "Fecha no disponible"))
                        })
                    else:
                        # Si el resultado es un string u otro tipo
                        fuentes.append({
                            "numero": i + 1,
                            "titulo": str(result)[:80] + "..." if len(str(result)) > 80 else str(result),
                            "url": "URL no disponible",
                            "fecha": "Fecha no disponible"
                        })
            
            # Si no hay search_results, intentar con citations (limitado a 3)
            elif "citations" in data and data["citations"]:
                for i, citation in enumerate(data["citations"][:3]):  # MÃ¡ximo 3 citas
                    if isinstance(citation, dict):
                        fuentes.append({
                            "numero": i + 1,
                            "titulo": citation.get("title", "TÃ­tulo no disponible")[:80] + "..." if len(citation.get("title", "")) > 80 else citation.get("title", "TÃ­tulo no disponible"),
                            "url": citation.get("url", "URL no disponible"),
                            "fecha": citation.get("date", "Fecha no disponible")
                        })
                    else:
                        # Las citas como strings suelen ser URLs
                        url_citation = str(citation)
                        fuentes.append({
                            "numero": i + 1,
                            "titulo": f"Fuente {i + 1}",
                            "url": url_citation,
                            "fecha": "Fecha no disponible"
                        })
            
            # Mostrar fuentes encontradas en detalle (mÃ¡ximo 3)
            if fuentes:
                print(f"ğŸ“š FUENTES ENCONTRADAS ({len(fuentes)}/3):")
                print(f"{'-'*80}")
                for fuente in fuentes:
                    print(f"  {fuente['numero']}. {fuente['titulo']}")
                    if fuente['url'] != "URL no disponible":
                        print(f"     ğŸ”— {fuente['url']}")
                    if fuente['fecha'] != "Fecha no disponible":
                        print(f"     ğŸ“… {fuente['fecha']}")
                    print()
                print(f"{'-'*80}")
            else:
                print("âš ï¸  No se encontraron fuentes especÃ­ficas")
            
            # Verificar si se realizaron bÃºsquedas web
            busquedas_realizadas = len(fuentes) > 0 or "based on" in content.lower() or "according to" in content.lower()
            
            print(f"âœ… BÃºsqueda completada en {tiempo_respuesta:.2f}s con {len(fuentes)} fuentes")
            
            return SonarResponse(
                contenido=content,
                fuentes=fuentes,
                busquedas_realizadas=busquedas_realizadas,
                tiempo_respuesta=tiempo_respuesta,
                modelo_usado=modelo
            )
        else:
            print(f"âŒ Error en Sonar - Status: {response.status_code}, Response: {response.text}")
            return SonarResponse(
                contenido="InformaciÃ³n no disponible en este momento",
                fuentes=[],
                busquedas_realizadas=False,
                tiempo_respuesta=tiempo_respuesta,
                modelo_usado=modelo
            )
            
    except requests.exceptions.Timeout:
        print("â° Timeout en Sonar API")
        return SonarResponse(
            contenido="InformaciÃ³n no disponible - timeout",
            fuentes=[],
            busquedas_realizadas=False,
            tiempo_respuesta=45.0,
            modelo_usado=modelo
        )
    except Exception as e:
        print(f"âŒ Error en Sonar: {e}")
        return SonarResponse(
            contenido="InformaciÃ³n no disponible",
            fuentes=[],
            busquedas_realizadas=False,
            tiempo_respuesta=0.0,
            modelo_usado="error"
        )

# Prompt integral para investigaciÃ³n completa de empresa y puesto
def crear_prompt_integral(empresa: str, puesto: str) -> str:
    """Busca informaciÃ³n completa sobre la empresa, cultura y puesto en una sola consulta"""
    return f"""Busca informaciÃ³n COMPLETA y detallada sobre {empresa} y el puesto {puesto}. Incluye TODA la informaciÃ³n necesaria para generar preguntas de entrevista contextualizadas:

INFORMACIÃ“N EMPRESARIAL COMPLETA:
- Historia, fundaciÃ³n y evoluciÃ³n de {empresa}
- Modelo de negocio, servicios principales y productos
- TamaÃ±o de la empresa (empleados, facturaciÃ³n, presencia global)
- TecnologÃ­as, plataformas y herramientas especÃ­ficas que utiliza {empresa}
- Proyectos actuales, iniciativas importantes y desarrollos recientes
- PosiciÃ³n en el mercado, competidores y sector de la industria
- Noticias recientes y desarrollos estratÃ©gicos (Ãºltimos 6-12 meses)

CULTURA ORGANIZACIONAL Y AMBIENTE LABORAL:
- Valores corporativos, principios y misiÃ³n de {empresa}
- MetodologÃ­as de trabajo y procesos (Ã¡gil, remoto, presencial, hÃ­brido)
- Beneficios especÃ­ficos y polÃ­ticas de recursos humanos
- Programas de desarrollo profesional y crecimiento
- Estilo de liderazgo y estructura organizacional
- Ambiente de trabajo y cultura de equipo
- Testimonios de empleados y experiencias laborales

CONTEXTO ESPECÃFICO DEL PUESTO {puesto}:
- Responsabilidades exactas del {puesto} en {empresa}
- TecnologÃ­as, herramientas y metodologÃ­as especÃ­ficas para este rol
- Estructura del equipo y colaboraciÃ³n interdepartamental
- Proyectos tÃ­picos y retos especÃ­ficos del {puesto} en {empresa}
- Perfil ideal y competencias buscadas por {empresa}
- Oportunidades de crecimiento y desarrollo profesional
- Contexto salarial y compensaciÃ³n en el mercado

FUENTES A CONSULTAR:
- PÃ¡gina web oficial de {empresa}
- LinkedIn (empresa y empleados)
- Glassdoor y plataformas de reseÃ±as laborales
- Ofertas laborales actuales de {empresa}
- Comunicados de prensa y noticias recientes
- Entrevistas con ejecutivos y empleados
- Informes de industria y anÃ¡lisis de mercado

Proporciona informaciÃ³n detallada, especÃ­fica y verificable que permita generar preguntas de entrevista contextualizadas con datos reales de {empresa}."""

# FunciÃ³n para generar preguntas con OpenAI
def generar_preguntas(propuesta: PropuestaLaboral, informacion_integral: str) -> dict:
    """Genera preguntas usando OpenAI con contexto integral enriquecido"""
    
    # Usar la informaciÃ³n integral obtenida de la bÃºsqueda Ãºnica
    info_completa = f"""
INFORMACIÃ“N INTEGRAL INVESTIGADA SOBRE {propuesta.empresa}:
{informacion_integral}
"""
    
    prompt = f"""
    Eres un experto en recursos humanos y entrevistas tÃ©cnicas especializadas.
    
    PROPUESTA LABORAL:
    - Empresa: {propuesta.empresa}
    - Puesto: {propuesta.puesto}
    - DescripciÃ³n: {propuesta.descripcion}
    - Requisitos: {propuesta.requisitos}
    
    CONTEXTO ESPECÃFICO INVESTIGADO:
    {info_completa}
    
    INSTRUCCIONES ESPECÃFICAS:
    BasÃ¡ndote DIRECTAMENTE en la informaciÃ³n especÃ­fica investigada sobre {propuesta.empresa}, genera:
    
    10-12 preguntas de EVALUACIÃ“N CONTEXTUALIZADAS para que el ENTREVISTADOR le haga al CANDIDATO:
    
    TIPOS DE PREGUNTAS CON CONTEXTO ESPECÃFICO:
    - 3-4 preguntas tÃ©cnicas que mencionen EXPLÃCITAMENTE las tecnologÃ­as, herramientas, plataformas o metodologÃ­as especÃ­ficas que usa {propuesta.empresa} segÃºn la investigaciÃ³n
    - 3-4 preguntas situacionales que incorporen DIRECTAMENTE los valores, cultura, metodologÃ­as de trabajo o retos especÃ­ficos mencionados en la investigaciÃ³n sobre {propuesta.empresa}
    - 4-5 preguntas de competencias que incluyan REFERENCIAS ESPECÃFICAS a proyectos reales, productos, servicios o contexto de mercado de {propuesta.empresa} encontrado en la investigaciÃ³n
    
    CÃ“MO POTENCIAR LAS PREGUNTAS CON CONTEXTO:
    - INCLUYE nombres especÃ­ficos de tecnologÃ­as, productos, plataformas que usa {propuesta.empresa}
    - MENCIONA metodologÃ­as, procesos o enfoques especÃ­ficos encontrados en la investigaciÃ³n
    - INCORPORA retos reales, proyectos actuales o iniciativas especÃ­ficas de {propuesta.empresa}
    - REFERENCIA la cultura, valores o ambiente de trabajo especÃ­fico identificado
    - USA el contexto de la industria, mercado o sector donde opera {propuesta.empresa}
    
    FORMATO DE PREGUNTAS CONTEXTUALIZADAS:
    - "En {propuesta.empresa} utilizamos [tecnologÃ­a especÃ­fica encontrada] para [contexto especÃ­fico]. Â¿CÃ³mo abordarÃ­as [situaciÃ³n tÃ©cnica]?"
    - "Considerando que {propuesta.empresa} [contexto cultural/organizacional especÃ­fico], describe cÃ³mo manejarÃ­as [situaciÃ³n]"
    - "Dado que {propuesta.empresa} estÃ¡ trabajando en [proyecto/iniciativa especÃ­fica encontrada], Â¿quÃ© estrategia usarÃ­as para [competencia especÃ­fica]?"
    
    REQUISITOS CRÃTICOS:
    - CADA pregunta DEBE incluir al menos UNA referencia especÃ­fica de la investigaciÃ³n
    - EVITA preguntas genÃ©ricas - todas deben estar contextualizadas con informaciÃ³n real de {propuesta.empresa}
    - INCORPORA datos concretos, nombres de productos, tecnologÃ­as, proyectos o metodologÃ­as encontradas
    - Las preguntas evalÃºan al candidato USANDO el contexto especÃ­fico como marco de referencia
    
    Responde en formato JSON con las siguientes claves:
    - "preguntas": lista de strings con las preguntas especÃ­ficas
    """
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Eres un experto en RRHH que genera preguntas de entrevista CONTEXTUALIZADAS y especÃ­ficas. SIEMPRE incluyes informaciÃ³n especÃ­fica de la empresa investigada en cada pregunta. Las preguntas deben incorporar tecnologÃ­as, proyectos, cultura y contexto real de la empresa. Responde SIEMPRE en formato JSON vÃ¡lido."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            response_format={"type": "json_object"}
        )
        
        import json
        resultado = json.loads(response.choices[0].message.content)
        
        # Mostrar preguntas generadas
        preguntas = resultado.get('preguntas', [])
        
        print(f"âœ… Preguntas contextualizadas generadas: {len(preguntas)} preguntas potenciadas con informaciÃ³n especÃ­fica de {propuesta.empresa}")
        
        if preguntas:
            print(f"\nğŸ¤” PREGUNTAS CONTEXTUALIZADAS CON INFORMACIÃ“N ESPECÃFICA DE {propuesta.empresa.upper()}:")
            print(f"{'='*80}")
            for i, pregunta in enumerate(preguntas, 1):
                print(f"  {i}. {pregunta}")
                print()
            print(f"{'='*80}")
        
        return resultado
    except Exception as e:
        print(f"âŒ Error en OpenAI: {e}")
        return {"preguntas": []}

@app.get("/")
async def root():
    return {
        "mensaje": "API de Entrevistas - Entre-Vistas", 
        "version": "3.1", 
        "funcionalidades": [
            "ğŸš€ SIEMPRE MÃXIMA CALIDAD (sonar-pro)",
            "1 BÃºsqueda integral ultra-completa",
            "MÃ¡ximo 3 fuentes de alta calidad",
            "2500 tokens para mÃ¡ximo detalle en una sola consulta",
            "InformaciÃ³n empresarial + cultura + puesto en bÃºsqueda unificada", 
            "Contexto completo e integrado de empresa y rol",
            "10-12 preguntas potenciadas con contexto integral especÃ­fico",
            "Referencias verificadas y de alta calidad",
            "Eficiencia optimizada con una sola consulta API"
        ],
        "configuracion_maxima_calidad": {
            "modelo": "sonar-pro",
            "tokens": 2500,
            "temperature": 0.1,
            "profundidad": "advanced",
            "fuentes_maximas": 3
        },
        "busqueda_integral": {
            "tipo": "BÃºsqueda completa unificada",
            "incluye": "Empresa + Cultura + Puesto + Contexto especÃ­fico",
            "fuentes": "MÃ¡ximo 3 fuentes de alta calidad",
            "eficiencia": "Una sola consulta API optimizada"
        },
        "metricas_calidad": {
            "Alta": "3 fuentes encontradas",
            "Media": "2 fuentes encontradas", 
            "Baja": "1 fuente encontrada"
        }
    }

@app.post("/test-sonar")
async def test_sonar(data: dict):
    """Endpoint de debug para verificar la bÃºsqueda integral de Sonar con MÃXIMA CALIDAD"""
    empresa = data.get("empresa", "Entel PerÃº")
    puesto = data.get("puesto", "IA Engineer")
    
    try:
        # BÃºsqueda integral unificada
        query = crear_prompt_integral(empresa, puesto)
        resultado = buscar_con_sonar(query)
        
        return {
            "tipo_busqueda": "integral_completa",
            "empresa": empresa,
            "puesto": puesto,
            "query_enviado": query,
            "resultado": {
                "contenido": resultado.contenido,
                "fuentes": resultado.fuentes,
                "busquedas_web_realizadas": resultado.busquedas_realizadas,
                "tiempo_respuesta_segundos": resultado.tiempo_respuesta,
                "modelo_usado": resultado.modelo_usado,
                "numero_fuentes": len(resultado.fuentes)
            },
            "verificacion_web": {
                "tiene_fuentes": len(resultado.fuentes) > 0,
                "busquedas_confirmadas": resultado.busquedas_realizadas,
                "fuentes_obtenidas": f"{len(resultado.fuentes)}/3",
                "calidad_busqueda": "Alta" if len(resultado.fuentes) >= 3 else "Media" if len(resultado.fuentes) >= 2 else "Baja",
                "tipo": "BÃºsqueda Integral (Empresa + Cultura + Puesto)"
            }
        }
        
    except Exception as e:
        return {"error": str(e)}

@app.post("/generar-entrevista", response_model=RespuestaEntrevista)
async def generar_entrevista(propuesta_texto: PropuestaLaboralTexto):
    """Endpoint principal para generar preguntas de entrevista desde texto libre con investigaciÃ³n web integral"""
    
    print(f"ğŸ“ Texto recibido: {propuesta_texto.texto[:100]}...")
    
    try:
        # 1. Extraer informaciÃ³n estructurada del texto con OpenAI
        propuesta = extraer_informacion_propuesta(propuesta_texto.texto)
        
        print("\nğŸ“‹ PROPUESTA LABORAL EXTRAÃDA:")
        print(f"{'='*80}")
        print(f"ğŸ¢ Empresa: {propuesta.empresa}")
        print(f"ğŸ’¼ Puesto: {propuesta.puesto}")
        print(f"ğŸ“„ DescripciÃ³n: {propuesta.descripcion[:200]}...")
        print(f"âš¡ Requisitos: {propuesta.requisitos[:200]}...")
        print(f"{'='*80}")
        print(f"âœ… Propuesta extraÃ­da: {propuesta.empresa} - {propuesta.puesto}")
        
        # 2. BÃºsqueda integral con Sonar - UNA SOLA bÃºsqueda completa con MÃXIMA CALIDAD
        print(f"\n{'='*80}")
        print("ğŸš€ INICIANDO BÃšSQUEDA INTEGRAL COMPLETA CON SONAR (MÃXIMA CALIDAD)")
        print("ğŸ”§ ConfiguraciÃ³n: sonar-pro | 2500 tokens | temp=0.1 | profundidad=advanced")
        print(f"{'='*80}")
        
        # BÃšSQUEDA INTEGRAL: InformaciÃ³n completa de empresa, cultura y puesto (MÃXIMA CALIDAD)
        print(f"\nğŸ” BÃšSQUEDA INTEGRAL: {propuesta.empresa} + {propuesta.puesto}")
        print(f"{'-'*80}")
        query_integral = crear_prompt_integral(propuesta.empresa, propuesta.puesto)
        info_integral = buscar_con_sonar(query_integral)
        
        # 3. Preparar informaciÃ³n integral para generaciÃ³n de preguntas
        informacion_completa = info_integral.contenido
        total_fuentes = len(info_integral.fuentes)
        tiempo_total = info_integral.tiempo_respuesta
        
        print("\nğŸ“Š RESUMEN DE LA BÃšSQUEDA INTEGRAL (MÃXIMA CALIDAD):")
        print(f"{'='*80}")
        print(f"ğŸ” BÃºsqueda Integral Completa ({total_fuentes}/3 fuentes): {tiempo_total:.2f}s - {info_integral.modelo_usado}")
        print("ğŸš€ ConfiguraciÃ³n: sonar-pro, 2500 tokens, temp=0.1")
        print("ğŸ“š InformaciÃ³n obtenida: Empresa + Cultura + Puesto en una sola consulta")
        print(f"ğŸ“š Total: {total_fuentes} fuentes especÃ­ficas en {tiempo_total:.2f}s")
        print(f"{'='*80}")
        print(f"ğŸ“š InvestigaciÃ³n integral completa con MÃXIMA CALIDAD: {total_fuentes} fuentes")
        
        # 4. Generar preguntas contextualizadas con OpenAI
        print(f"\n{'='*80}")
        print("ğŸ’¡ GENERANDO PREGUNTAS CONTEXTUALIZADAS CON INFORMACIÃ“N INTEGRAL DE LA EMPRESA")
        print(f"{'='*80}")
        resultado = generar_preguntas(propuesta, informacion_completa)
        
        # 5. Extraer preguntas del resultado
        preguntas = resultado.get("preguntas", [])
        
        # 6. Construir respuesta completa
        respuesta = RespuestaEntrevista(
            preguntas=preguntas,
            informacion_empresa={
                "nombre": propuesta.empresa,
                "informacion_encontrada": info_integral.contenido[:800] + "..." if len(info_integral.contenido) > 800 else info_integral.contenido,
                "fuentes_consultadas": len(info_integral.fuentes),
                "busquedas_web_verificadas": info_integral.busquedas_realizadas
            },
            propuesta_extraida={
                "empresa": propuesta.empresa,
                "puesto": propuesta.puesto,
                "descripcion": propuesta.descripcion,
                "requisitos": propuesta.requisitos
            },
            investigacion_detallada={
                "resumen_fuentes": {
                    "busqueda_integral": {"total": len(info_integral.fuentes), "modelo": info_integral.modelo_usado}
                },
                "calidad_investigacion": "Alta" if total_fuentes >= 3 else "Media" if total_fuentes >= 2 else "Baja",
                "busqueda_web_realizada": info_integral.busquedas_realizadas,
                "tiempo_total": tiempo_total,
                "tipo_busqueda": "BÃºsqueda Integral Completa (Empresa + Cultura + Puesto)"
            }
        )
        
        print(f"\n{'='*80}")
        print("ğŸ‰ PROCESO COMPLETADO EXITOSAMENTE CON MÃXIMA CALIDAD")
        print(f"{'='*80}")
        print("ğŸ“Š Resultados finales:")
        print(f"   â€¢ {len(preguntas)} preguntas contextualizadas con informaciÃ³n integral de la empresa")
        print(f"   â€¢ {total_fuentes}/3 fuentes de alta calidad consultadas en una sola bÃºsqueda")
        print(f"   â€¢ Tiempo total: {tiempo_total:.2f} segundos")
        print("   â€¢ ConfiguraciÃ³n: sonar-pro, 2500 tokens en bÃºsqueda integral")
        print(f"   â€¢ Calidad investigaciÃ³n: {'Alta' if total_fuentes >= 3 else 'Media' if total_fuentes >= 2 else 'Baja'}")
        print(f"{'='*80}")
        print(f"ğŸš€ Respuesta construida con MÃXIMA CALIDAD: {len(preguntas)} preguntas, {total_fuentes} fuentes")
        return respuesta
        
    except Exception as e:
        print(f"âŒ Error en generar_entrevista: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error procesando la solicitud: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000) 