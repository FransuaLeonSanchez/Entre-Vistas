from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Optional
from openai import OpenAI
import requests
import aiohttp
import asyncio
import os
from dotenv import load_dotenv
from datetime import datetime

# Cargar variables de entorno
load_dotenv()

app = FastAPI()

# Configurar CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Configurar clientes API
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
SONAR_API_KEY = os.getenv("SONAR_API_KEY")

# Modelos de datos
class PropuestaLaboralTexto(BaseModel):
    texto: str

class PropuestaLaboralConOpciones(BaseModel):
    texto: str
    buscar_empresa: bool = True
    buscar_puesto_mercado: bool = False  
    buscar_entrevistador: bool = False
    nombre_entrevistador: Optional[str] = None

class PropuestaLaboral(BaseModel):
    empresa: str
    puesto: str
    descripcion: str
    requisitos: str

class SonarResponse(BaseModel):
    contenido: str
    fuentes: List[Dict]
    busquedas_realizadas: bool
    tiempo_respuesta: float
    modelo_usado: str

class RespuestaEntrevista(BaseModel):
    preguntas: List[str]
    consejos_conexion: List[str] = []
    informacion_empresa: Dict
    propuesta_extraida: Dict
    investigacion_detallada: Dict

# FunciÃ³n para extraer informaciÃ³n de la propuesta
def extraer_informacion_propuesta(texto: str) -> PropuestaLaboral:
    """Extrae informaciÃ³n estructurada de un texto de propuesta laboral usando OpenAI"""
    
    prompt = f"""
    Analiza el siguiente texto de una propuesta laboral y extrae la informaciÃ³n estructurada.
    
    Texto de la propuesta:
    {texto}
    
    Extrae y responde en formato JSON con las siguientes claves:
    - "empresa": nombre de la empresa
    - "puesto": tÃ­tulo del puesto
    - "descripcion": descripciÃ³n del trabajo y responsabilidades
    - "requisitos": concatena toda la informaciÃ³n de requisitos, formaciÃ³n, experiencia y competencias en un solo texto
    
    Para el campo "requisitos", incluye toda la informaciÃ³n relevante en un pÃ¡rrafo continuo.
    """
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Eres un experto en anÃ¡lisis de propuestas laborales. Siempre responde en formato JSON vÃ¡lido con los campos exactos solicitados."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            response_format={"type": "json_object"}
        )
        
        import json
        datos = json.loads(response.choices[0].message.content)
        
        print(f"âœ… Datos extraÃ­dos: {datos}")
        
        return PropuestaLaboral(**datos)
    except Exception as e:
        print(f"âŒ Error extrayendo informaciÃ³n: {e}")
        return PropuestaLaboral(
            empresa="Empresa no identificada",
            puesto="Puesto no identificado", 
            descripcion=texto[:200],
            requisitos="No especificados"
        )

# FunciÃ³n optimizada para bÃºsquedas con Sonar - SIEMPRE MÃXIMA CALIDAD
def buscar_con_sonar(query: str, search_type: str = "pro") -> SonarResponse:
    """Busca informaciÃ³n usando la API de Sonar de Perplexity con MÃXIMA CALIDAD siempre"""
    
    start_time = datetime.now()
    url = "https://api.perplexity.ai/chat/completions"
    
    headers = {
        "Authorization": f"Bearer {SONAR_API_KEY}",
        "Content-Type": "application/json"
    }
    
    # ConfiguraciÃ³n SIEMPRE de mÃ¡xima calidad
    modelo = "sonar-pro"
    max_tokens = 2500  # Aumentado para mÃ¡xima calidad
    temperature = 0.1  # MÃ¡s preciso para bÃºsquedas especÃ­ficas
    
    payload = {
        "model": modelo,
        "messages": [
            {
                "role": "system",
                "content": "Eres un investigador experto de Ã©lite que proporciona informaciÃ³n actualizada, precisa y extremadamente detallada basada en fuentes web confiables de alta calidad. Prioriza fuentes oficiales, verificables y recientes. Incluye detalles especÃ­ficos, datos concretos y contexto relevante en cada respuesta."
            },
            {
                "role": "user", 
                "content": query
            }
        ],
        "max_tokens": max_tokens,
        "temperature": temperature,
        # "search_domain_filter": []  # Permitir bÃºsquedas en toda la web
        "search_recency_filter": "month",  # Ãšltimas bÃºsquedas del mes
        "return_related_questions": True,
        "search_depth_filter": "advanced",  # BÃºsqueda mÃ¡s profunda
        "enable_clarification_questions": True  # Preguntas de clarificaciÃ³n para mejor contexto
    }
    
    try:
        if not SONAR_API_KEY:
            print("âŒ SONAR_API_KEY no configurada")
            return SonarResponse(
                contenido="InformaciÃ³n no disponible - API key no configurada",
                fuentes=[],
                busquedas_realizadas=False,
                tiempo_respuesta=0.0,
                modelo_usado="none"
            )
            
        print(f"ğŸ” Iniciando bÃºsqueda con {modelo} (MÃXIMA CALIDAD)")
        print(f"ğŸ“ Prompt enviado: {query[:150]}...")
        print(f"âš¡ ConfiguraciÃ³n: {max_tokens} tokens, temp={temperature}")
        print(f"â±ï¸  Esperando respuesta...")
        
        response = requests.post(url, json=payload, headers=headers, timeout=45)
        tiempo_respuesta = (datetime.now() - start_time).total_seconds()
        
        if response.status_code == 200:
            data = response.json()
            print(f"ğŸ” Debug - Claves en respuesta: {list(data.keys())}")
            
            # Validar estructura de respuesta
            if "choices" not in data or not data["choices"]:
                print("âŒ Error: Respuesta sin campo 'choices'")
                return SonarResponse(
                    contenido="Error en estructura de respuesta",
                    fuentes=[],
                    busquedas_realizadas=False,
                    tiempo_respuesta=tiempo_respuesta,
                    modelo_usado=modelo
                )
            
            content = data["choices"][0]["message"]["content"]
            
            # Mostrar el contenido de la respuesta en consola
            print(f"ğŸ“„ CONTENIDO DE LA BÃšSQUEDA:")
            print(f"{'='*80}")
            print(content[:500] + "..." if len(content) > 500 else content)
            print(f"{'='*80}")
            
            # Extraer fuentes/citas de la respuesta
            fuentes = []
            
            # Debug: mostrar estructura de datos adicionales (limitado a 3 fuentes)
            if "search_results" in data:
                print(f"ğŸ” Debug - Encontrados {len(data['search_results'])} resultados (usando mÃ¡ximo 3)")
                if data["search_results"]:
                    print(f"ğŸ” Debug - Primer tipo de resultado: {type(data['search_results'][0])}")
            
            if "citations" in data:
                print(f"ğŸ” Debug - Encontradas {len(data['citations'])} citas (usando mÃ¡ximo 3)")
                if data["citations"]:
                    print(f"ğŸ” Debug - Primer tipo de cita: {type(data['citations'][0])}")
            
            # Extraer fuentes de search_results (limitado a 3 fuentes)
            if "search_results" in data and data["search_results"]:
                for i, result in enumerate(data["search_results"][:3]):  # MÃ¡ximo 3 fuentes
                    if isinstance(result, dict):
                        fuentes.append({
                            "numero": i + 1,
                            "titulo": result.get("title", result.get("name", "TÃ­tulo no disponible"))[:80] + "..." if len(result.get("title", result.get("name", ""))) > 80 else result.get("title", result.get("name", "TÃ­tulo no disponible")),
                            "url": result.get("url", "URL no disponible"),
                            "fecha": result.get("date", result.get("published_date", "Fecha no disponible"))
                        })
                    else:
                        # Si el resultado es un string u otro tipo
                        fuentes.append({
                            "numero": i + 1,
                            "titulo": str(result)[:80] + "..." if len(str(result)) > 80 else str(result),
                            "url": "URL no disponible",
                            "fecha": "Fecha no disponible"
                        })
            
            # Si no hay search_results, intentar con citations (limitado a 3)
            elif "citations" in data and data["citations"]:
                for i, citation in enumerate(data["citations"][:3]):  # MÃ¡ximo 3 citas
                    if isinstance(citation, dict):
                        fuentes.append({
                            "numero": i + 1,
                            "titulo": citation.get("title", "TÃ­tulo no disponible")[:80] + "..." if len(citation.get("title", "")) > 80 else citation.get("title", "TÃ­tulo no disponible"),
                            "url": citation.get("url", "URL no disponible"),
                            "fecha": citation.get("date", "Fecha no disponible")
                        })
                    else:
                        # Las citas como strings suelen ser URLs
                        url_citation = str(citation)
                        fuentes.append({
                            "numero": i + 1,
                            "titulo": f"Fuente {i + 1}",
                            "url": url_citation,
                            "fecha": "Fecha no disponible"
                        })
            
            # Mostrar fuentes encontradas en detalle (mÃ¡ximo 3)
            if fuentes:
                print(f"ğŸ“š FUENTES ENCONTRADAS ({len(fuentes)}/3):")
                print(f"{'-'*80}")
                for fuente in fuentes:
                    print(f"  {fuente['numero']}. {fuente['titulo']}")
                    if fuente['url'] != "URL no disponible":
                        print(f"     ğŸ”— {fuente['url']}")
                    if fuente['fecha'] != "Fecha no disponible":
                        print(f"     ğŸ“… {fuente['fecha']}")
                    print()
                print(f"{'-'*80}")
            else:
                print("âš ï¸  No se encontraron fuentes especÃ­ficas")
            
            # Verificar si se realizaron bÃºsquedas web
            busquedas_realizadas = len(fuentes) > 0 or "based on" in content.lower() or "according to" in content.lower()
            
            print(f"âœ… BÃºsqueda completada en {tiempo_respuesta:.2f}s con {len(fuentes)} fuentes")
            
            return SonarResponse(
                contenido=content,
                fuentes=fuentes,
                busquedas_realizadas=busquedas_realizadas,
                tiempo_respuesta=tiempo_respuesta,
                modelo_usado=modelo
            )
        else:
            print(f"âŒ Error en Sonar - Status: {response.status_code}, Response: {response.text}")
            return SonarResponse(
                contenido="InformaciÃ³n no disponible en este momento",
                fuentes=[],
                busquedas_realizadas=False,
                tiempo_respuesta=tiempo_respuesta,
                modelo_usado=modelo
            )
            
    except requests.exceptions.Timeout:
        print("â° Timeout en Sonar API")
        return SonarResponse(
            contenido="InformaciÃ³n no disponible - timeout",
            fuentes=[],
            busquedas_realizadas=False,
            tiempo_respuesta=45.0,
            modelo_usado=modelo
        )
    except Exception as e:
        print(f"âŒ Error en Sonar: {e}")
        return SonarResponse(
            contenido="InformaciÃ³n no disponible",
            fuentes=[],
            busquedas_realizadas=False,
            tiempo_respuesta=0.0,
            modelo_usado="error"
        )

# FunciÃ³n asÃ­ncrona para bÃºsquedas paralelas con Sonar
async def buscar_con_sonar_async(query: str, search_type: str = "pro") -> SonarResponse:
    """VersiÃ³n asÃ­ncrona de buscar_con_sonar para paralelizaciÃ³n"""
    
    start_time = datetime.now()
    url = "https://api.perplexity.ai/chat/completions"
    
    headers = {
        "Authorization": f"Bearer {SONAR_API_KEY}",
        "Content-Type": "application/json"
    }
    
    # ConfiguraciÃ³n SIEMPRE de mÃ¡xima calidad
    modelo = "sonar-pro"
    max_tokens = 2500
    temperature = 0.1
    
    payload = {
        "model": modelo,
        "messages": [
            {
                "role": "system",
                "content": "Eres un investigador experto de Ã©lite que proporciona informaciÃ³n actualizada, precisa y extremadamente detallada basada en fuentes web confiables de alta calidad. Prioriza fuentes oficiales, verificables y recientes. Incluye detalles especÃ­ficos, datos concretos y contexto relevante en cada respuesta."
            },
            {
                "role": "user", 
                "content": query
            }
        ],
        "max_tokens": max_tokens,
        "temperature": temperature,
        "search_recency_filter": "month",
        "return_related_questions": True,
        "search_depth_filter": "advanced",
        "enable_clarification_questions": True
    }
    
    try:
        if not SONAR_API_KEY:
            print("âŒ SONAR_API_KEY no configurada")
            return SonarResponse(
                contenido="InformaciÃ³n no disponible - API key no configurada",
                fuentes=[],
                busquedas_realizadas=False,
                tiempo_respuesta=0.0,
                modelo_usado="none"
            )
            
        print(f"ğŸ” Iniciando bÃºsqueda con {modelo} (MÃXIMA CALIDAD - ASYNC)")
        print(f"ğŸ“ Prompt enviado: {query[:150]}...")
        
        timeout = aiohttp.ClientTimeout(total=45)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.post(url, json=payload, headers=headers) as response:
                tiempo_respuesta = (datetime.now() - start_time).total_seconds()
                
                if response.status == 200:
                    data = await response.json()
                    print(f"ğŸ” Debug - Claves en respuesta: {list(data.keys())}")
                    
                    # Validar estructura de respuesta
                    if "choices" not in data or not data["choices"]:
                        print("âŒ Error: Respuesta sin campo 'choices'")
                        return SonarResponse(
                            contenido="Error en estructura de respuesta",
                            fuentes=[],
                            busquedas_realizadas=False,
                            tiempo_respuesta=tiempo_respuesta,
                            modelo_usado=modelo
                        )
                    
                    content = data["choices"][0]["message"]["content"]
                    
                    # Extraer fuentes/citas de la respuesta
                    fuentes = []
                    
                    # Extraer fuentes de search_results (limitado a 3 fuentes)
                    if "search_results" in data and data["search_results"]:
                        for i, result in enumerate(data["search_results"][:3]):
                            if isinstance(result, dict):
                                fuentes.append({
                                    "numero": i + 1,
                                    "titulo": result.get("title", result.get("name", "TÃ­tulo no disponible"))[:80] + "..." if len(result.get("title", result.get("name", ""))) > 80 else result.get("title", result.get("name", "TÃ­tulo no disponible")),
                                    "url": result.get("url", "URL no disponible"),
                                    "fecha": result.get("date", result.get("published_date", "Fecha no disponible"))
                                })
                            else:
                                fuentes.append({
                                    "numero": i + 1,
                                    "titulo": str(result)[:80] + "..." if len(str(result)) > 80 else str(result),
                                    "url": "URL no disponible",
                                    "fecha": "Fecha no disponible"
                                })
                    
                    # Si no hay search_results, intentar con citations (limitado a 3)
                    elif "citations" in data and data["citations"]:
                        for i, citation in enumerate(data["citations"][:3]):
                            if isinstance(citation, dict):
                                fuentes.append({
                                    "numero": i + 1,
                                    "titulo": citation.get("title", "TÃ­tulo no disponible")[:80] + "..." if len(citation.get("title", "")) > 80 else citation.get("title", "TÃ­tulo no disponible"),
                                    "url": citation.get("url", "URL no disponible"),
                                    "fecha": citation.get("date", "Fecha no disponible")
                                })
                            else:
                                url_citation = str(citation)
                                fuentes.append({
                                    "numero": i + 1,
                                    "titulo": f"Fuente {i + 1}",
                                    "url": url_citation,
                                    "fecha": "Fecha no disponible"
                                })
                    
                    # Verificar si se realizaron bÃºsquedas web
                    busquedas_realizadas = len(fuentes) > 0 or "based on" in content.lower() or "according to" in content.lower()
                    
                    print(f"âœ… BÃºsqueda async completada en {tiempo_respuesta:.2f}s con {len(fuentes)} fuentes")
                    
                    return SonarResponse(
                        contenido=content,
                        fuentes=fuentes,
                        busquedas_realizadas=busquedas_realizadas,
                        tiempo_respuesta=tiempo_respuesta,
                        modelo_usado=modelo
                    )
                else:
                    response_text = await response.text()
                    print(f"âŒ Error en Sonar - Status: {response.status}, Response: {response_text}")
                    return SonarResponse(
                        contenido="InformaciÃ³n no disponible en este momento",
                        fuentes=[],
                        busquedas_realizadas=False,
                        tiempo_respuesta=tiempo_respuesta,
                        modelo_usado=modelo
                    )
                    
    except asyncio.TimeoutError:
        print("â° Timeout en Sonar API (async)")
        return SonarResponse(
            contenido="InformaciÃ³n no disponible - timeout",
            fuentes=[],
            busquedas_realizadas=False,
            tiempo_respuesta=45.0,
            modelo_usado=modelo
        )
    except Exception as e:
        print(f"âŒ Error en Sonar async: {e}")
        return SonarResponse(
            contenido="InformaciÃ³n no disponible",
            fuentes=[],
            busquedas_realizadas=False,
            tiempo_respuesta=0.0,
            modelo_usado="error"
        )

# FunciÃ³n para ejecutar bÃºsquedas en paralelo
async def ejecutar_busquedas_paralelas(busquedas: List[tuple]) -> List[SonarResponse]:
    """
    Ejecuta mÃºltiples bÃºsquedas de Sonar en paralelo
    busquedas: Lista de tuplas (query, nombre_busqueda)
    """
    print(f"\nâš¡ EJECUTANDO {len(busquedas)} BÃšSQUEDAS EN PARALELO")
    print(f"{'='*80}")
    
    start_time = datetime.now()
    
    # Crear las tareas asÃ­ncronas
    tasks = []
    for query, nombre in busquedas:
        print(f"ğŸš€ Preparando bÃºsqueda: {nombre}")
        task = buscar_con_sonar_async(query)
        tasks.append(task)
    
    # Ejecutar todas las bÃºsquedas en paralelo
    print(f"âš¡ Iniciando {len(tasks)} bÃºsquedas simultÃ¡neas...")
    resultados = await asyncio.gather(*tasks)
    
    tiempo_total = (datetime.now() - start_time).total_seconds()
    print(f"âœ… TODAS LAS BÃšSQUEDAS COMPLETADAS EN {tiempo_total:.2f}s")
    print(f"{'='*80}")
    
    return resultados

# Prompt integral para investigaciÃ³n completa de empresa y puesto
def crear_prompt_integral(empresa: str, puesto: str) -> str:
    """Busca informaciÃ³n completa sobre la empresa, cultura y puesto en una sola consulta"""
    return f"""Busca informaciÃ³n COMPLETA y detallada sobre {empresa} y el puesto {puesto}. Incluye TODA la informaciÃ³n necesaria para generar preguntas de entrevista contextualizadas:

INFORMACIÃ“N EMPRESARIAL COMPLETA:
- Historia, fundaciÃ³n y evoluciÃ³n de {empresa}
- Modelo de negocio, servicios principales y productos
- TamaÃ±o de la empresa (empleados, facturaciÃ³n, presencia global)
- TecnologÃ­as, plataformas y herramientas especÃ­ficas que utiliza {empresa}
- Proyectos actuales, iniciativas importantes y desarrollos recientes
- PosiciÃ³n en el mercado, competidores y sector de la industria
- Noticias recientes y desarrollos estratÃ©gicos (Ãºltimos 6-12 meses)

CULTURA ORGANIZACIONAL Y AMBIENTE LABORAL:
- Valores corporativos, principios y misiÃ³n de {empresa}
- MetodologÃ­as de trabajo y procesos (Ã¡gil, remoto, presencial, hÃ­brido)
- Beneficios especÃ­ficos y polÃ­ticas de recursos humanos
- Programas de desarrollo profesional y crecimiento
- Estilo de liderazgo y estructura organizacional
- Ambiente de trabajo y cultura de equipo
- Testimonios de empleados y experiencias laborales

CONTEXTO ESPECÃFICO DEL PUESTO {puesto}:
- Responsabilidades exactas del {puesto} en {empresa}
- TecnologÃ­as, herramientas y metodologÃ­as especÃ­ficas para este rol
- Estructura del equipo y colaboraciÃ³n interdepartamental
- Proyectos tÃ­picos y retos especÃ­ficos del {puesto} en {empresa}
- Perfil ideal y competencias buscadas por {empresa}
- Oportunidades de crecimiento y desarrollo profesional
- Contexto salarial y compensaciÃ³n en el mercado

FUENTES A CONSULTAR:
- PÃ¡gina web oficial de {empresa}
- LinkedIn (empresa y empleados)
- Glassdoor y plataformas de reseÃ±as laborales
- Ofertas laborales actuales de {empresa}
- Comunicados de prensa y noticias recientes
- Entrevistas con ejecutivos y empleados
- Informes de industria y anÃ¡lisis de mercado

Proporciona informaciÃ³n detallada, especÃ­fica y verificable que permita generar preguntas de entrevista contextualizadas con datos reales de {empresa}."""

# FunciÃ³n especÃ­fica para bÃºsqueda de informaciÃ³n de empresa
def crear_prompt_empresa(empresa: str, puesto: str) -> str:
    """Busca informaciÃ³n especÃ­fica sobre la empresa en PerÃº - contexto, tecnologÃ­as, cultura"""
    return f"""Busca informaciÃ³n DETALLADA y especÃ­fica sobre {empresa} en PerÃº, enfocÃ¡ndote en el contexto para entrevistas del puesto {puesto}:

INFORMACIÃ“N EMPRESARIAL EN PERÃš:
- Historia y presencia de {empresa} en PerÃº especÃ­ficamente
- Operaciones, servicios y productos de {empresa} en el mercado peruano
- TamaÃ±o de operaciones en PerÃº (empleados, oficinas, proyectos)
- TecnologÃ­as, plataformas y herramientas especÃ­ficas que utiliza {empresa} en PerÃº
- Stack tecnolÃ³gico, metodologÃ­as de desarrollo y arquitectura tÃ©cnica
- Proyectos actuales y desarrollos importantes en PerÃº
- Clientes principales y sectores que atiende en PerÃº

CULTURA Y AMBIENTE LABORAL EN PERÃš:
- Valores corporativos y cultura organizacional de {empresa} en PerÃº
- MetodologÃ­as de trabajo (Ã¡gil, DevOps, frameworks especÃ­ficos)
- Modalidad de trabajo (remoto, hÃ­brido, presencial) en oficinas peruanas
- Beneficios y polÃ­ticas especÃ­ficas para empleados en PerÃº
- Programas de capacitaciÃ³n y desarrollo profesional
- Ambiente de trabajo y testimonios de empleados en PerÃº

CONTEXTO ESPECÃFICO DEL PUESTO {puesto}:
- CÃ³mo opera el Ã¡rea de {puesto} dentro de {empresa} en PerÃº
- TecnologÃ­as y herramientas especÃ­ficas para {puesto} en {empresa}
- Proyectos tÃ­picos y responsabilidades del {puesto} en {empresa}
- Perfil buscado y competencias valoradas por {empresa} para {puesto}

NOTICIAS Y DESARROLLOS RECIENTES:
- Noticias recientes de {empresa} en PerÃº (Ãºltimos 6-12 meses)
- Expansiones, nuevos proyectos o iniciativas en PerÃº
- Comunicados de prensa y desarrollos estratÃ©gicos

EnfÃ³cate en informaciÃ³n verificable y especÃ­fica que permita generar preguntas contextualizadas para una entrevista de {puesto} en {empresa}."""

# FunciÃ³n especÃ­fica para bÃºsqueda de mercado laboral del puesto
def crear_prompt_puesto_mercado(puesto: str) -> str:
    """Busca informaciÃ³n sobre puestos similares en otras empresas de PerÃº para contexto de mercado"""
    return f"""Busca informaciÃ³n sobre el mercado laboral del puesto {puesto} en PerÃº, incluyendo otras empresas y contexto sectorial:

ANÃLISIS DE MERCADO DEL PUESTO {puesto} EN PERÃš:
- Principales empresas en PerÃº que contratan para {puesto}
- TecnologÃ­as mÃ¡s demandadas para {puesto} en el mercado peruano
- Habilidades y competencias mÃ¡s valoradas en PerÃº para {puesto}
- Rangos salariales y compensaciÃ³n tÃ­pica para {puesto} en PerÃº
- Tendencias actuales del mercado laboral para {puesto}

EMPRESAS REFERENCIALES EN PERÃš:
- Principales empresas tecnolÃ³gicas/consultoras que contratan {puesto}
- Startups y empresas emergentes con posiciones de {puesto}
- Corporaciones multinacionales con operaciones en PerÃº
- Modalidades de trabajo mÃ¡s comunes (remoto, hÃ­brido, presencial)

COMPETENCIAS Y TECNOLOGÃAS EN DEMANDA:
- Stack tecnolÃ³gico mÃ¡s solicitado para {puesto} en PerÃº
- Certificaciones y habilidades tÃ©cnicas valoradas
- Soft skills y competencias blandas importantes
- MetodologÃ­as y frameworks mÃ¡s utilizados

CONTEXTO SECTORIAL:
- Sectores de la industria que mÃ¡s demandan {puesto} en PerÃº
- Proyectos tÃ­picos y retos comunes en {puesto}
- Oportunidades de crecimiento profesional en el mercado peruano
- Tendencias de transformaciÃ³n digital que afectan {puesto}

Esta informaciÃ³n ayudarÃ¡ a entender el contexto competitivo y las expectativas del mercado para {puesto} en PerÃº."""

# FunciÃ³n especÃ­fica para bÃºsqueda de informaciÃ³n personal del entrevistador
def crear_prompt_entrevistador_personal(nombre_entrevistador: str, empresa: str) -> str:
    """Busca informaciÃ³n general sobre el entrevistador para establecer conexiÃ³n"""
    return f"""Â¿QuÃ© sabes de {nombre_entrevistador}? Busca en internet toda la informaciÃ³n posible sobre esta persona de cualquier tipo.

Busca informaciÃ³n general y completa sobre {nombre_entrevistador}:

- Cualquier informaciÃ³n personal, profesional o pÃºblica disponible
- Perfil en redes sociales (LinkedIn, Twitter, Instagram, etc.)
- ArtÃ­culos, publicaciones, entrevistas o contenido que haya creado
- ParticipaciÃ³n en eventos, conferencias o charlas
- EducaciÃ³n, experiencia laboral y trayectoria
- Intereses, hobbies o actividades personales
- Proyectos, logros o reconocimientos
- Personalidad, estilo de comunicaciÃ³n y valores
- Cualquier otra informaciÃ³n relevante que encuentres

Proporciona toda la informaciÃ³n disponible sobre {nombre_entrevistador} para entender su personalidad y estilo como entrevistador."""

# FunciÃ³n para generar preguntas con OpenAI
def generar_preguntas(propuesta: PropuestaLaboral, informacion_integral: str) -> dict:
    """Genera preguntas usando OpenAI con contexto integral enriquecido"""
    
    # Usar la informaciÃ³n integral obtenida de la bÃºsqueda Ãºnica
    info_completa = f"""
INFORMACIÃ“N INTEGRAL INVESTIGADA SOBRE {propuesta.empresa}:
{informacion_integral}
"""
    
    prompt = f"""
    Eres un experto en recursos humanos y entrevistas tÃ©cnicas especializadas.
    
    PROPUESTA LABORAL:
    - Empresa: {propuesta.empresa}
    - Puesto: {propuesta.puesto}
    - DescripciÃ³n: {propuesta.descripcion}
    - Requisitos: {propuesta.requisitos}
    
    CONTEXTO ESPECÃFICO INVESTIGADO:
    {info_completa}
    
    INSTRUCCIONES ESPECÃFICAS:
    BasÃ¡ndote DIRECTAMENTE en la informaciÃ³n especÃ­fica investigada sobre {propuesta.empresa}, genera:
    
    10-12 preguntas de EVALUACIÃ“N CONTEXTUALIZADAS para que el ENTREVISTADOR le haga al CANDIDATO:
    
    TIPOS DE PREGUNTAS CON CONTEXTO ESPECÃFICO:
    - 3-4 preguntas tÃ©cnicas que mencionen EXPLÃCITAMENTE las tecnologÃ­as, herramientas, plataformas o metodologÃ­as especÃ­ficas que usa {propuesta.empresa} segÃºn la investigaciÃ³n
    - 3-4 preguntas situacionales que incorporen DIRECTAMENTE los valores, cultura, metodologÃ­as de trabajo o retos especÃ­ficos mencionados en la investigaciÃ³n sobre {propuesta.empresa}
    - 4-5 preguntas de competencias que incluyan REFERENCIAS ESPECÃFICAS a proyectos reales, productos, servicios o contexto de mercado de {propuesta.empresa} encontrado en la investigaciÃ³n
    
    CÃ“MO POTENCIAR LAS PREGUNTAS CON CONTEXTO:
    - INCLUYE nombres especÃ­ficos de tecnologÃ­as, productos, plataformas que usa {propuesta.empresa}
    - MENCIONA metodologÃ­as, procesos o enfoques especÃ­ficos encontrados en la investigaciÃ³n
    - INCORPORA retos reales, proyectos actuales o iniciativas especÃ­ficas de {propuesta.empresa}
    - REFERENCIA la cultura, valores o ambiente de trabajo especÃ­fico identificado
    - USA el contexto de la industria, mercado o sector donde opera {propuesta.empresa}
    
    FORMATO DE PREGUNTAS CONTEXTUALIZADAS:
    - "En {propuesta.empresa} utilizamos [tecnologÃ­a especÃ­fica encontrada] para [contexto especÃ­fico]. Â¿CÃ³mo abordarÃ­as [situaciÃ³n tÃ©cnica]?"
    - "Considerando que {propuesta.empresa} [contexto cultural/organizacional especÃ­fico], describe cÃ³mo manejarÃ­as [situaciÃ³n]"
    - "Dado que {propuesta.empresa} estÃ¡ trabajando en [proyecto/iniciativa especÃ­fica encontrada], Â¿quÃ© estrategia usarÃ­as para [competencia especÃ­fica]?"
    
    REQUISITOS CRÃTICOS:
    - CADA pregunta DEBE incluir al menos UNA referencia especÃ­fica de la investigaciÃ³n
    - EVITA preguntas genÃ©ricas - todas deben estar contextualizadas con informaciÃ³n real de {propuesta.empresa}
    - INCORPORA datos concretos, nombres de productos, tecnologÃ­as, proyectos o metodologÃ­as encontradas
    - Las preguntas evalÃºan al candidato USANDO el contexto especÃ­fico como marco de referencia
    
    Responde ÃšNICAMENTE en formato JSON vÃ¡lido con esta estructura exacta:
    {{
        "preguntas": [
            "texto de pregunta 1 como string",
            "texto de pregunta 2 como string",
            ...
        ]
    }}
    
    IMPORTANTE: Las preguntas deben ser strings directos, NO objetos con llaves.
    """
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Eres un experto en RRHH que genera preguntas de entrevista CONTEXTUALIZADAS y especÃ­ficas. SIEMPRE incluyes informaciÃ³n especÃ­fica de la empresa investigada en cada pregunta. Las preguntas deben incorporar tecnologÃ­as, proyectos, cultura y contexto real de la empresa. Responde SIEMPRE en formato JSON vÃ¡lido con arrays de strings, NUNCA objetos con llaves."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            response_format={"type": "json_object"}
        )
        
        import json
        resultado = json.loads(response.choices[0].message.content)
        
        # Procesar preguntas para asegurar formato correcto
        preguntas_raw = resultado.get('preguntas', [])
        preguntas = []
        
        for pregunta in preguntas_raw:
            if isinstance(pregunta, dict):
                # Si OpenAI devolviÃ³ un objeto, extraer el texto de la pregunta
                texto_pregunta = pregunta.get('pregunta', str(pregunta))
                preguntas.append(texto_pregunta)
            elif isinstance(pregunta, str):
                # Si ya es string, usarlo directamente
                preguntas.append(pregunta)
            else:
                # Convertir a string como fallback
                preguntas.append(str(pregunta))
        
        print(f"âœ… Preguntas contextualizadas generadas: {len(preguntas)} preguntas potenciadas con informaciÃ³n especÃ­fica de {propuesta.empresa}")
        
        if preguntas:
            print(f"\nğŸ¤” PREGUNTAS CONTEXTUALIZADAS CON INFORMACIÃ“N ESPECÃFICA DE {propuesta.empresa.upper()}:")
            print(f"{'='*80}")
            for i, pregunta in enumerate(preguntas, 1):
                print(f"  {i}. {pregunta}")
                print()
            print(f"{'='*80}")
        
        # Devolver resultado con formato corregido
        return {"preguntas": preguntas}
    except Exception as e:
        print(f"âŒ Error en OpenAI: {e}")
        return {"preguntas": []}

# FunciÃ³n para generar preguntas con informaciÃ³n mÃºltiple (empresa + mercado + entrevistador)
def generar_preguntas_con_contexto_multiple(propuesta: PropuestaLaboral, info_empresa: str = "", info_mercado: str = "", info_entrevistador: str = "") -> dict:
    """Genera preguntas usando OpenAI con mÃºltiples contextos de informaciÃ³n"""
    
    # Construir informaciÃ³n contextual combinada
    contexto_completo = ""
    
    if info_empresa:
        contexto_completo += f"""
INFORMACIÃ“N ESPECÃFICA DE {propuesta.empresa}:
{info_empresa}

"""
    
    if info_mercado:
        contexto_completo += f"""
CONTEXTO DEL MERCADO LABORAL PARA {propuesta.puesto} EN PERÃš:
{info_mercado}

"""
    
    if info_entrevistador:
        contexto_completo += f"""
INFORMACIÃ“N PERSONAL DEL ENTREVISTADOR:
{info_entrevistador}

"""
    
    prompt = f"""
    Eres un experto en recursos humanos y entrevistas tÃ©cnicas especializadas.
    
    PROPUESTA LABORAL:
    - Empresa: {propuesta.empresa}
    - Puesto: {propuesta.puesto}
    - DescripciÃ³n: {propuesta.descripcion}
    - Requisitos: {propuesta.requisitos}
    
    CONTEXTO INVESTIGADO:
    {contexto_completo}
    
    INSTRUCCIONES ESPECÃFICAS:
    Genera 10-12 preguntas de EVALUACIÃ“N CONTEXTUALIZADAS basÃ¡ndote PRINCIPALMENTE en la informaciÃ³n de la EMPRESA y el MERCADO LABORAL:
    
    FUENTES PRINCIPALES PARA LAS PREGUNTAS:
    - InformaciÃ³n especÃ­fica de {propuesta.empresa}: tecnologÃ­as, cultura, proyectos, metodologÃ­as
    - AnÃ¡lisis del mercado laboral para {propuesta.puesto}: tendencias, competencias demandadas, tecnologÃ­as populares
    
    TIPOS DE PREGUNTAS:
    - 4-5 preguntas tÃ©cnicas incorporando tecnologÃ­as especÃ­ficas de {propuesta.empresa} y tecnologÃ­as demandadas en el mercado
    - 3-4 preguntas situacionales basadas en la cultura y metodologÃ­as de {propuesta.empresa}
    - 3-4 preguntas de competencias que combinen los retos especÃ­ficos de {propuesta.empresa} con las competencias valoradas en el mercado
    
    CÃ“MO ESTRUCTURAR LAS PREGUNTAS:
    - PRIORIZA informaciÃ³n especÃ­fica de {propuesta.empresa} (tecnologÃ­as, proyectos, cultura, metodologÃ­as)
    - INCORPORA tendencias y competencias del mercado laboral para {propuesta.puesto} en PerÃº
    - MENCIONA herramientas, frameworks y tecnologÃ­as especÃ­ficas encontradas
    - REFERENCIA proyectos actuales, retos reales y contexto especÃ­fico de {propuesta.empresa}
    
    USO DE LA INFORMACIÃ“N DEL ENTREVISTADOR:
    La informaciÃ³n del entrevistador es SOLO para contexto y consejos de conexiÃ³n personal, NO para generar las preguntas principales.
    - Si hay informaciÃ³n del entrevistador, Ãºsala para generar consejos de conexiÃ³n
    - Adapta el tono/estilo de las preguntas segÃºn la personalidad del entrevistador
    - Sugiere temas de conversaciÃ³n casual basados en sus intereses
    
    REQUISITOS:
    - Las preguntas deben evaluar competencias tÃ©cnicas y profesionales especÃ­ficas
    - Basar TODAS las preguntas en informaciÃ³n concreta de empresa + mercado
    - Evita preguntas genÃ©ricas o que no tengan contexto especÃ­fico
    - Genera consejos separados para establecer conexiÃ³n personal con el entrevistador
    
    Responde ÃšNICAMENTE en formato JSON vÃ¡lido con esta estructura exacta:
    {{
        "preguntas": [
            "texto de pregunta 1 como string",
            "texto de pregunta 2 como string",
            ...
        ],
        "consejos_conexion": [
            "consejo 1 como string",
            "consejo 2 como string",
            ...
        ]
    }}
    
    IMPORTANTE: Las preguntas deben ser strings directos, NO objetos con llaves.
    """
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Eres un experto en RRHH que genera preguntas contextualizadas combinando informaciÃ³n de empresa, mercado y entrevistador. Creas conexiones humanas autÃ©nticas. Responde SIEMPRE en formato JSON vÃ¡lido con arrays de strings, NUNCA objetos con llaves."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            response_format={"type": "json_object"}
        )
        
        import json
        resultado = json.loads(response.choices[0].message.content)
        
        # Procesar preguntas para asegurar formato correcto
        preguntas_raw = resultado.get('preguntas', [])
        preguntas = []
        
        for pregunta in preguntas_raw:
            if isinstance(pregunta, dict):
                # Si OpenAI devolviÃ³ un objeto, extraer el texto de la pregunta
                texto_pregunta = pregunta.get('pregunta', str(pregunta))
                preguntas.append(texto_pregunta)
            elif isinstance(pregunta, str):
                # Si ya es string, usarlo directamente
                preguntas.append(pregunta)
            else:
                # Convertir a string como fallback
                preguntas.append(str(pregunta))
        
        # Procesar consejos de conexiÃ³n
        consejos_raw = resultado.get('consejos_conexion', [])
        consejos = []
        
        for consejo in consejos_raw:
            if isinstance(consejo, dict):
                # Si es objeto, extraer el texto
                texto_consejo = consejo.get('consejo', str(consejo))
                consejos.append(texto_consejo)
            elif isinstance(consejo, str):
                # Si ya es string, usarlo directamente
                consejos.append(consejo)
            else:
                # Convertir a string como fallback
                consejos.append(str(consejo))
        
        print(f"âœ… Preguntas contextualizadas generadas: {len(preguntas)} preguntas con contexto mÃºltiple")
        if consejos:
            print(f"âœ… Consejos de conexiÃ³n personal generados: {len(consejos)} consejos")
        
        # Devolver resultado con formato corregido
        return {
            "preguntas": preguntas,
            "consejos_conexion": consejos
        }
    except Exception as e:
        print(f"âŒ Error en OpenAI: {e}")
        return {"preguntas": [], "consejos_conexion": []}

@app.get("/")
async def root():
    return {
        "mensaje": "API de Entrevistas - Entre-Vistas", 
        "version": "4.0", 
        "funcionalidades": [
            "ğŸš€ SISTEMA DE BÃšSQUEDAS PARALELAS CON MÃXIMA CALIDAD",
            "âš¡ BÃºsquedas simultÃ¡neas para mÃ¡ximo rendimiento",
            "3 tipos de bÃºsquedas independientes activables por parÃ¡metros",
            "BÃºsqueda de empresa: contexto, tecnologÃ­as, cultura en PerÃº",
            "BÃºsqueda de mercado: puestos similares en otras empresas de PerÃº", 
            "BÃºsqueda de entrevistador: informaciÃ³n personal para conexiÃ³n humana",
            "sonar-pro con 2500 tokens para cada bÃºsqueda activada",
            "10-12 preguntas contextualizadas con informaciÃ³n combinada",
            "Consejos de conexiÃ³n personal con el entrevistador"
        ],
        "configuracion_maxima_calidad": {
            "modelo": "sonar-pro",
            "tokens": 2500,
            "temperature": 0.1,
            "profundidad": "advanced",
            "fuentes_maximas_por_busqueda": 3
        },
        "busquedas_opcionales": {
            "buscar_empresa": "InformaciÃ³n especÃ­fica de la empresa en PerÃº (true/false)",
            "buscar_puesto_mercado": "AnÃ¡lisis del puesto en otras empresas de PerÃº (true/false)",
            "buscar_entrevistador": "InformaciÃ³n personal del entrevistador para conexiÃ³n (true/false)",
            "nombre_entrevistador": "Nombre del entrevistador (opcional, requerido si buscar_entrevistador=true)"
        },
        "endpoints": {
            "/generar-entrevista": "Endpoint original (bÃºsqueda integral)",
            "/generar-entrevista-con-opciones": "Nuevo endpoint con bÃºsquedas opcionales",
            "/test-sonar": "Endpoint de prueba para bÃºsqueda integral",
            "/test-busquedas-opcionales": "Endpoint de prueba para bÃºsquedas opcionales"
        },
        "ejemplo_uso": {
            "texto": "Propuesta laboral como texto",
            "buscar_empresa": True,
            "buscar_puesto_mercado": True, 
            "buscar_entrevistador": False,
            "nombre_entrevistador": None
        }
    }

@app.post("/test-sonar")
async def test_sonar(data: dict):
    """Endpoint de debug para verificar la bÃºsqueda integral de Sonar con MÃXIMA CALIDAD"""
    empresa = data.get("empresa", "Entel PerÃº")
    puesto = data.get("puesto", "IA Engineer")
    
    try:
        # BÃºsqueda integral unificada
        query = crear_prompt_integral(empresa, puesto)
        resultado = buscar_con_sonar(query)
        
        return {
            "tipo_busqueda": "integral_completa",
            "empresa": empresa,
            "puesto": puesto,
            "query_enviado": query,
            "resultado": {
                "contenido": resultado.contenido,
                "fuentes": resultado.fuentes,
                "busquedas_web_realizadas": resultado.busquedas_realizadas,
                "tiempo_respuesta_segundos": resultado.tiempo_respuesta,
                "modelo_usado": resultado.modelo_usado,
                "numero_fuentes": len(resultado.fuentes)
            },
            "verificacion_web": {
                "tiene_fuentes": len(resultado.fuentes) > 0,
                "busquedas_confirmadas": resultado.busquedas_realizadas,
                "fuentes_obtenidas": f"{len(resultado.fuentes)}/3",
                "calidad_busqueda": "Alta" if len(resultado.fuentes) >= 3 else "Media" if len(resultado.fuentes) >= 2 else "Baja",
                "tipo": "BÃºsqueda Integral (Empresa + Cultura + Puesto)"
            }
        }
        
    except Exception as e:
        return {"error": str(e)}

@app.post("/test-busquedas-opcionales")
async def test_busquedas_opcionales(data: dict):
    """Endpoint de debug para verificar las bÃºsquedas opcionales con MÃXIMA CALIDAD"""
    empresa = data.get("empresa", "Entel PerÃº")
    puesto = data.get("puesto", "IA Engineer")
    nombre_entrevistador = data.get("nombre_entrevistador", None)
    
    # Opciones de bÃºsqueda (por defecto todas activadas para test)
    buscar_empresa = data.get("buscar_empresa", True)
    buscar_mercado = data.get("buscar_puesto_mercado", True)
    buscar_entrevistador = data.get("buscar_entrevistador", False)
    
    try:
        resultados = {}
        
        # Test bÃºsqueda de empresa
        if buscar_empresa:
            print(f"ğŸ” Testing bÃºsqueda de empresa: {empresa}")
            query_empresa = crear_prompt_empresa(empresa, puesto)
            resultado_empresa = buscar_con_sonar(query_empresa)
            resultados["empresa"] = {
                "query_enviado": query_empresa,
                "contenido": resultado_empresa.contenido,
                "fuentes": resultado_empresa.fuentes,
                "tiempo_respuesta": resultado_empresa.tiempo_respuesta,
                "modelo_usado": resultado_empresa.modelo_usado,
                "numero_fuentes": len(resultado_empresa.fuentes)
            }
        
        # Test bÃºsqueda de mercado
        if buscar_mercado:
            print(f"ğŸ” Testing bÃºsqueda de mercado: {puesto}")
            query_mercado = crear_prompt_puesto_mercado(puesto)
            resultado_mercado = buscar_con_sonar(query_mercado)
            resultados["mercado"] = {
                "query_enviado": query_mercado,
                "contenido": resultado_mercado.contenido,
                "fuentes": resultado_mercado.fuentes,
                "tiempo_respuesta": resultado_mercado.tiempo_respuesta,
                "modelo_usado": resultado_mercado.modelo_usado,
                "numero_fuentes": len(resultado_mercado.fuentes)
            }
        
        # Test bÃºsqueda de entrevistador
        if buscar_entrevistador and nombre_entrevistador:
            print(f"ğŸ” Testing bÃºsqueda de entrevistador: {nombre_entrevistador}")
            query_entrevistador = crear_prompt_entrevistador_personal(nombre_entrevistador, empresa)
            resultado_entrevistador = buscar_con_sonar(query_entrevistador)
            resultados["entrevistador"] = {
                "query_enviado": query_entrevistador,
                "contenido": resultado_entrevistador.contenido,
                "fuentes": resultado_entrevistador.fuentes,
                "tiempo_respuesta": resultado_entrevistador.tiempo_respuesta,
                "modelo_usado": resultado_entrevistador.modelo_usado,
                "numero_fuentes": len(resultado_entrevistador.fuentes)
            }
        
        # Calcular estadÃ­sticas totales
        total_fuentes = sum(resultado.get("numero_fuentes", 0) for resultado in resultados.values())
        tiempo_total = sum(resultado.get("tiempo_respuesta", 0) for resultado in resultados.values())
        
        return {
            "tipo_busqueda": "opcionales_separadas",
            "parametros": {
                "empresa": empresa,
                "puesto": puesto,
                "nombre_entrevistador": nombre_entrevistador,
                "buscar_empresa": buscar_empresa,
                "buscar_mercado": buscar_mercado,
                "buscar_entrevistador": buscar_entrevistador
            },
            "resultados": resultados,
            "estadisticas": {
                "busquedas_realizadas": len(resultados),
                "total_fuentes": total_fuentes,
                "tiempo_total": tiempo_total,
                "calidad_investigacion": "Alta" if total_fuentes >= 6 else "Media" if total_fuentes >= 3 else "Baja"
            }
        }
        
    except Exception as e:
        return {"error": str(e)}

@app.post("/generar-entrevista", response_model=RespuestaEntrevista)
async def generar_entrevista(propuesta_texto: PropuestaLaboralTexto):
    """Endpoint principal para generar preguntas de entrevista desde texto libre con investigaciÃ³n web integral"""
    
    print(f"ğŸ“ Texto recibido: {propuesta_texto.texto[:100]}...")
    
    try:
        # 1. Extraer informaciÃ³n estructurada del texto con OpenAI
        propuesta = extraer_informacion_propuesta(propuesta_texto.texto)
        
        print(f"\nğŸ“‹ PROPUESTA LABORAL EXTRAÃDA:")
        print(f"{'='*80}")
        print(f"ğŸ¢ Empresa: {propuesta.empresa}")
        print(f"ğŸ’¼ Puesto: {propuesta.puesto}")
        print(f"ğŸ“„ DescripciÃ³n: {propuesta.descripcion[:200]}...")
        print(f"âš¡ Requisitos: {propuesta.requisitos[:200]}...")
        print(f"{'='*80}")
        print(f"âœ… Propuesta extraÃ­da: {propuesta.empresa} - {propuesta.puesto}")
        
        # 2. BÃºsqueda integral con Sonar - UNA SOLA bÃºsqueda completa con MÃXIMA CALIDAD
        print(f"\n{'='*80}")
        print("ğŸš€ INICIANDO BÃšSQUEDA INTEGRAL COMPLETA CON SONAR (MÃXIMA CALIDAD)")
        print("ğŸ”§ ConfiguraciÃ³n: sonar-pro | 2500 tokens | temp=0.1 | profundidad=advanced")
        print(f"{'='*80}")
        
        # BÃšSQUEDA INTEGRAL: InformaciÃ³n completa de empresa, cultura y puesto (MÃXIMA CALIDAD)
        print(f"\nğŸ” BÃšSQUEDA INTEGRAL: {propuesta.empresa} + {propuesta.puesto}")
        print(f"{'-'*80}")
        query_integral = crear_prompt_integral(propuesta.empresa, propuesta.puesto)
        info_integral = buscar_con_sonar(query_integral)
        
        # 3. Preparar informaciÃ³n integral para generaciÃ³n de preguntas
        informacion_completa = info_integral.contenido
        total_fuentes = len(info_integral.fuentes)
        tiempo_total = info_integral.tiempo_respuesta
        
        print(f"\nğŸ“Š RESUMEN DE LA BÃšSQUEDA INTEGRAL (MÃXIMA CALIDAD):")
        print(f"{'='*80}")
        print(f"ğŸ” BÃºsqueda Integral Completa ({total_fuentes}/3 fuentes): {tiempo_total:.2f}s - {info_integral.modelo_usado}")
        print(f"ğŸš€ ConfiguraciÃ³n: sonar-pro, 2500 tokens, temp=0.1")
        print(f"ğŸ“š InformaciÃ³n obtenida: Empresa + Cultura + Puesto en una sola consulta")
        print(f"ğŸ“š Total: {total_fuentes} fuentes especÃ­ficas en {tiempo_total:.2f}s")
        print(f"{'='*80}")
        print(f"ğŸ“š InvestigaciÃ³n integral completa con MÃXIMA CALIDAD: {total_fuentes} fuentes")
        
        # 4. Generar preguntas contextualizadas con OpenAI
        print(f"\n{'='*80}")
        print("ğŸ’¡ GENERANDO PREGUNTAS CONTEXTUALIZADAS CON INFORMACIÃ“N INTEGRAL DE LA EMPRESA")
        print(f"{'='*80}")
        resultado = generar_preguntas(propuesta, informacion_completa)
        
        # 5. Extraer preguntas del resultado
        preguntas = resultado.get("preguntas", [])
        
        # 6. Construir respuesta completa
        respuesta = RespuestaEntrevista(
            preguntas=preguntas,
            consejos_conexion=[],  # No aplicable en bÃºsqueda integral
            informacion_empresa={
                "nombre": propuesta.empresa,
                "informacion_encontrada": info_integral.contenido[:800] + "..." if len(info_integral.contenido) > 800 else info_integral.contenido,
                "fuentes_consultadas": len(info_integral.fuentes),
                "busquedas_web_verificadas": info_integral.busquedas_realizadas
            },
            propuesta_extraida={
                "empresa": propuesta.empresa,
                "puesto": propuesta.puesto,
                "descripcion": propuesta.descripcion,
                "requisitos": propuesta.requisitos
            },
            investigacion_detallada={
                "resumen_fuentes": {
                    "busqueda_integral": {"total": len(info_integral.fuentes), "modelo": info_integral.modelo_usado}
                },
                "calidad_investigacion": "Alta" if total_fuentes >= 3 else "Media" if total_fuentes >= 2 else "Baja",
                "busqueda_web_realizada": info_integral.busquedas_realizadas,
                "tiempo_total": tiempo_total,
                "tipo_busqueda": "BÃºsqueda Integral Completa (Empresa + Cultura + Puesto)"
            }
        )
        
        print(f"\n{'='*80}")
        print("ğŸ‰ PROCESO COMPLETADO EXITOSAMENTE CON MÃXIMA CALIDAD")
        print(f"{'='*80}")
        print(f"ğŸ“Š Resultados finales:")
        print(f"   â€¢ {len(preguntas)} preguntas contextualizadas con informaciÃ³n integral de la empresa")
        print(f"   â€¢ {total_fuentes}/3 fuentes de alta calidad consultadas en una sola bÃºsqueda")
        print(f"   â€¢ Tiempo total: {tiempo_total:.2f} segundos")
        print(f"   â€¢ ConfiguraciÃ³n: sonar-pro, 2500 tokens en bÃºsqueda integral")
        print(f"   â€¢ Calidad investigaciÃ³n: {'Alta' if total_fuentes >= 3 else 'Media' if total_fuentes >= 2 else 'Baja'}")
        print(f"{'='*80}")
        print(f"ğŸš€ Respuesta construida con MÃXIMA CALIDAD: {len(preguntas)} preguntas, {total_fuentes} fuentes")
        return respuesta
        
    except Exception as e:
        print(f"âŒ Error en generar_entrevista: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error procesando la solicitud: {str(e)}")

@app.post("/generar-entrevista-con-opciones", response_model=RespuestaEntrevista)
async def generar_entrevista_con_opciones(propuesta_opciones: PropuestaLaboralConOpciones):
    """Endpoint principal con bÃºsquedas opcionales (empresa, mercado, reclutador) usando mÃ¡xima calidad"""
    
    print(f"ğŸ“ Texto recibido: {propuesta_opciones.texto[:100]}...")
    print(f"ğŸ” Opciones de bÃºsqueda:")
    print(f"   â€¢ Buscar empresa: {propuesta_opciones.buscar_empresa}")
    print(f"   â€¢ Buscar mercado: {propuesta_opciones.buscar_puesto_mercado}")
    print(f"   â€¢ Buscar entrevistador: {propuesta_opciones.buscar_entrevistador}")
    if propuesta_opciones.nombre_entrevistador:
        print(f"   â€¢ Nombre entrevistador: {propuesta_opciones.nombre_entrevistador}")
    
    try:
        # 1. Extraer informaciÃ³n estructurada del texto con OpenAI
        propuesta = extraer_informacion_propuesta(propuesta_opciones.texto)
        
        print(f"\nğŸ“‹ PROPUESTA LABORAL EXTRAÃDA:")
        print(f"{'='*80}")
        print(f"ğŸ¢ Empresa: {propuesta.empresa}")
        print(f"ğŸ’¼ Puesto: {propuesta.puesto}")
        print(f"ğŸ“„ DescripciÃ³n: {propuesta.descripcion[:200]}...")
        print(f"âš¡ Requisitos: {propuesta.requisitos[:200]}...")
        print(f"{'='*80}")
        
        # 2. Preparar bÃºsquedas opcionales para paralelizaciÃ³n
        busquedas_pendientes = []
        info_empresa = ""
        info_mercado = ""
        info_entrevistador = ""
        
        print(f"\n{'='*80}")
        print("ğŸš€ INICIANDO BÃšSQUEDAS OPCIONALES PARALELAS CON MÃXIMA CALIDAD")
        print("ğŸ”§ ConfiguraciÃ³n: sonar-pro | 2500 tokens | temp=0.1 | profundidad=advanced")
        print(f"{'='*80}")
        
        # Preparar bÃºsquedas segÃºn configuraciÃ³n
        if propuesta_opciones.buscar_empresa:
            query_empresa = crear_prompt_empresa(propuesta.empresa, propuesta.puesto)
            busquedas_pendientes.append((query_empresa, f"EMPRESA ({propuesta.empresa})"))
            
        if propuesta_opciones.buscar_puesto_mercado:
            query_mercado = crear_prompt_puesto_mercado(propuesta.puesto)
            busquedas_pendientes.append((query_mercado, f"MERCADO ({propuesta.puesto})"))
            
        if propuesta_opciones.buscar_entrevistador and propuesta_opciones.nombre_entrevistador:
            query_entrevistador = crear_prompt_entrevistador_personal(propuesta_opciones.nombre_entrevistador, propuesta.empresa)
            busquedas_pendientes.append((query_entrevistador, f"ENTREVISTADOR ({propuesta_opciones.nombre_entrevistador})"))
        elif propuesta_opciones.buscar_entrevistador and not propuesta_opciones.nombre_entrevistador:
            print(f"\nâš ï¸  BÃšSQUEDA ENTREVISTADOR ACTIVADA PERO SIN NOMBRE - OMITIDA")
        
        # Ejecutar bÃºsquedas en paralelo
        resultados = []
        busquedas_realizadas = []
        total_fuentes = 0
        
        if busquedas_pendientes:
            resultados = await ejecutar_busquedas_paralelas(busquedas_pendientes)
            
            # Procesar resultados y asignar informaciÃ³n
            for i, (query, nombre) in enumerate(busquedas_pendientes):
                resultado = resultados[i]
                
                if "EMPRESA" in nombre:
                    info_empresa = resultado.contenido
                    busquedas_realizadas.append({
                        "tipo": "empresa",
                        "fuentes": len(resultado.fuentes),
                        "tiempo": resultado.tiempo_respuesta,
                        "modelo": resultado.modelo_usado
                    })
                elif "MERCADO" in nombre:
                    info_mercado = resultado.contenido
                    busquedas_realizadas.append({
                        "tipo": "mercado",
                        "fuentes": len(resultado.fuentes),
                        "tiempo": resultado.tiempo_respuesta,
                        "modelo": resultado.modelo_usado
                    })
                elif "ENTREVISTADOR" in nombre:
                    info_entrevistador = resultado.contenido
                    busquedas_realizadas.append({
                        "tipo": "entrevistador",
                        "fuentes": len(resultado.fuentes),
                        "tiempo": resultado.tiempo_respuesta,
                        "modelo": resultado.modelo_usado
                    })
                
                total_fuentes += len(resultado.fuentes)
        
        # Calcular tiempo total (el tiempo real de la paralelizaciÃ³n)
        tiempo_total = max([b["tiempo"] for b in busquedas_realizadas]) if busquedas_realizadas else 0.0
        
        # 3. Resumen de bÃºsquedas realizadas
        print(f"\nğŸ“Š RESUMEN DE BÃšSQUEDAS PARALELAS (MÃXIMA CALIDAD):")
        print(f"{'='*80}")
        tiempo_individual_total = 0.0
        for busqueda in busquedas_realizadas:
            print(f"ğŸ” {busqueda['tipo'].title()}: {busqueda['fuentes']}/3 fuentes - {busqueda['tiempo']:.2f}s - {busqueda['modelo']}")
            tiempo_individual_total += busqueda['tiempo']
        
        if busquedas_realizadas:
            ahorro_tiempo = tiempo_individual_total - tiempo_total
            porcentaje_ahorro = (ahorro_tiempo / tiempo_individual_total * 100) if tiempo_individual_total > 0 else 0
            print(f"ğŸ“š Total: {total_fuentes} fuentes en {tiempo_total:.2f}s ({len(busquedas_realizadas)} bÃºsquedas PARALELAS)")
            print(f"âš¡ Ahorro de tiempo: {ahorro_tiempo:.2f}s ({porcentaje_ahorro:.1f}% mÃ¡s rÃ¡pido que secuencial)")
        print(f"{'='*80}")
        
        # 4. Generar preguntas contextualizadas con mÃºltiple informaciÃ³n
        print(f"\n{'='*80}")
        print("ğŸ’¡ GENERANDO PREGUNTAS CONTEXTUALIZADAS CON INFORMACIÃ“N COMBINADA")
        print(f"{'='*80}")
        resultado = generar_preguntas_con_contexto_multiple(
            propuesta, 
            info_empresa, 
            info_mercado, 
            info_entrevistador
        )
        
        # 5. Extraer preguntas y consejos del resultado
        preguntas = resultado.get("preguntas", [])
        consejos_conexion = resultado.get("consejos_conexion", [])
        
        # 6. Construir respuesta completa
        respuesta = RespuestaEntrevista(
            preguntas=preguntas,
            consejos_conexion=consejos_conexion,
            informacion_empresa={
                "nombre": propuesta.empresa,
                "informacion_encontrada": info_empresa[:800] + "..." if len(info_empresa) > 800 else info_empresa,
                "fuentes_consultadas": len([b for b in busquedas_realizadas if b["tipo"] == "empresa"]),
                "busquedas_web_verificadas": any(b["fuentes"] > 0 for b in busquedas_realizadas)
            },
            propuesta_extraida={
                "empresa": propuesta.empresa,
                "puesto": propuesta.puesto,
                "descripcion": propuesta.descripcion,
                "requisitos": propuesta.requisitos
            },
            investigacion_detallada={
                "busquedas_opcionales": {
                    "empresa": {"activada": propuesta_opciones.buscar_empresa, "fuentes": len([b for b in busquedas_realizadas if b["tipo"] == "empresa"])},
                    "mercado": {"activada": propuesta_opciones.buscar_puesto_mercado, "fuentes": len([b for b in busquedas_realizadas if b["tipo"] == "mercado"])},
                    "entrevistador": {"activada": propuesta_opciones.buscar_entrevistador, "fuentes": len([b for b in busquedas_realizadas if b["tipo"] == "entrevistador"])}
                },
                "calidad_investigacion": "Alta" if total_fuentes >= 6 else "Media" if total_fuentes >= 3 else "Baja",
                "busqueda_web_realizada": len(busquedas_realizadas) > 0,
                "tiempo_total": tiempo_total,
                "total_fuentes": total_fuentes,
                "busquedas_completadas": len(busquedas_realizadas),
                "resultados_busquedas": {
                    "empresa": {
                        "contenido": info_empresa[:1000] + "..." if len(info_empresa) > 1000 else info_empresa,
                        "activada": propuesta_opciones.buscar_empresa,
                        "fuentes": len([b for b in busquedas_realizadas if b["tipo"] == "empresa"]),
                        "tiempo": next((b["tiempo"] for b in busquedas_realizadas if b["tipo"] == "empresa"), 0.0)
                    },
                    "mercado": {
                        "contenido": info_mercado[:1000] + "..." if len(info_mercado) > 1000 else info_mercado,
                        "activada": propuesta_opciones.buscar_puesto_mercado,
                        "fuentes": len([b for b in busquedas_realizadas if b["tipo"] == "mercado"]),
                        "tiempo": next((b["tiempo"] for b in busquedas_realizadas if b["tipo"] == "mercado"), 0.0)
                    },
                    "entrevistador": {
                        "contenido": info_entrevistador[:1000] + "..." if len(info_entrevistador) > 1000 else info_entrevistador,
                        "activada": propuesta_opciones.buscar_entrevistador,
                        "fuentes": len([b for b in busquedas_realizadas if b["tipo"] == "entrevistador"]),
                        "tiempo": next((b["tiempo"] for b in busquedas_realizadas if b["tipo"] == "entrevistador"), 0.0)
                    }
                }
            }
        )
        
        print(f"\n{'='*80}")
        print("ğŸ‰ PROCESO COMPLETADO EXITOSAMENTE CON BÃšSQUEDAS PARALELAS")
        print(f"{'='*80}")
        print(f"ğŸ“Š Resultados finales:")
        print(f"   â€¢ {len(preguntas)} preguntas contextualizadas con informaciÃ³n combinada")
        print(f"   â€¢ {len(consejos_conexion)} consejos de conexiÃ³n personal generados")
        print(f"   â€¢ {total_fuentes} fuentes de alta calidad consultadas en {len(busquedas_realizadas)} bÃºsquedas PARALELAS")
        print(f"   â€¢ Tiempo total: {tiempo_total:.2f} segundos (paralelizaciÃ³n optimizada)")
        print(f"   â€¢ ConfiguraciÃ³n: sonar-pro, 2500 tokens por bÃºsqueda")
        print(f"   â€¢ Calidad investigaciÃ³n: {'Alta' if total_fuentes >= 6 else 'Media' if total_fuentes >= 3 else 'Baja'}")
        print(f"{'='*80}")
        
        return respuesta
        
    except Exception as e:
        print(f"âŒ Error en generar_entrevista_con_opciones: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error procesando la solicitud: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000) 